00:00你那是你们行，是这没有吗？没有，是给他说某个水清呢，之前发给我们，不是我我们天松宝地理，
02:45他这个就没声音。你把学信网里还有不经意老公没听过，鼓励一下局。但是是一直今天我们上课 OK 呃，上节课呃前面一几节课对吧？我们呃介绍了主要是介绍了存储啊，
05:40就是当你做数据管理的时候，它可能会出现的各种各样的问题。那今天的话我们会把这个话题稍微换一换，对吧？我们今天啊主要来谈网络啊，主要来谈一下 RDARDV。
05:52如果大家最近对这个这个这个这个之前对这个这个这个 AI inpro 的这块比较了解的话，可能或多或少都听说过，对吧？来来 remodecp number ASS 啊，它基本是一种新的这样的一个网络的 temunage。但是在现现在这个 AI 的这个时代非常非常需要啊，
06:09为为什么非非常常要要？其实这个网络非常这个这个技术其实非常的也不不能说。其实非常古老啊，其实最早追溯到一九八零年的时候，在这个超算，比如说这个这个中国的这种长沙的这种超超算中心啊，
06:24其实就就就就已经有了一九八零年数还还是在国内。它其实干的事情就是我这个分布式性应用，对吧？不管是计算也好，存储也好，它需要这个通信啊通信。
06:35那我怎么让这个通信做的快啊，他基本上就干这件事儿啊，但是要干成这件事其实并不是啊并不是这么容易的，所以他一直一直在演进。然后它的成本其实在前十年前，其实还是非常高的，
06:47很难在数据中心普及。但是啊就我我比如说我读我读博士的时候，写 RD 杯，在数据中心基本上是很很很很少会用得上。那最近啊不同啊，最近为什么呀？
06:56最近因为 AI 的这个出现，对吧？它对于这个这个计算能力上去之后，通信能力就要成比例的这个上去。然后 AI 其实说实话现在已经有感觉有点泡沫，对吧？
07:06就是有疯狂的资本涌入，然后导致的是这个这个这个这个资本非常多。那资本非常多的话，那么之前比较昂贵的这种 RDV 网络啊，在这种比较更昂贵的这个 GPU 面前，其实可变得这个一文不值啊，
07:20相当一文不值。所以说啊其实在现在的这个数据中心的话，已经啊开始啊大大大面积的这样的铺开。所以我觉得这个也是一个很好的时候去介绍一下这个 RDV 这个技术，对吧？如果大家以后去大厂里做 info，
07:32对吧？你要知道它它为什么能做到比传统的，比如说这个 TCI 会快，对吧？ g 到底我该去怎么用它？其实这个其实还是啊从现在这个时代来看的话，
07:40还是啊非常适合的 OK 行。那么我们之前在这个做这个 skilling 的啊，这个就是做分布式系统的课。我我们讲到一个分布式系统，或者说现在大现在主流系统，它的一个核心的特征就是它它是它是它走，
07:57它就是一个分布式系统。什么意思？就是说它的计算会分布在很多台机器上啊，每台机器需要互相通信啊，才能够完成一次计算也好，完成一次存储也好。
08:07那么这个规模在现在的数据中心已经开始涨到这个十万台这样的一个二十万台，甚至更多的这样的规模。那就会促促促使一个问题就是我的这么大的一个规模的机器，对吧？要怎么通信？我们说在网络里面其实是个非常非常非常重要的这样一个 topic。
08:24那虽然啊小红中有有人调侃，对吧？这种网络这种英法，包括系统英法里面像这个储物工程。但实际上我觉得这个讲法其实不对的。这个里面这个这里面其实有非常多的门道的。
08:34像系统里面的话，其实有专门的网络的这样的 conference c com 和 NSII。就就专门去讨论这样的一个问题，包括系统会有一大堆讨论这个网络。那么其实我们这边也可以看到，就是说这个这个现代，
08:47比如说你要把十万台机器连起来，它的这个网络实际上是非常非常复杂的。有这样一个海 ercky 的结构啊，有在 rack 内啊，我是一个全连接的。然后我在 rock 间有这样的一个交换机，
08:58交换机间上面还有一个顶层的这样的一个交换机。当然现在网络就会更加更加复杂。因为啊我的一个 rock，它的这个这个这个又分成 scale up 和 skill out。当然这个这个我们可能有时间再再介绍一下。行，
09:10那么要把这么大的一个规模网络起来跑起来，其实硬件是不够的。因为我们可可以看到的一个事情，就之前包括我们讲封闭系统也提到过，你那么大一个规模的网络，很容易就会出现丢包啊，
09:21或者说你的包会出错，然后或者你的包会乱序啊，甚至是你会出现这种网络分区啊，这些问题的话，你要在硬件实现不是说不可以。但是的话它本身的复杂度啊是相对来说是比较高的 OK。
09:36所以在这个时候呢，就是说你纯纯有一个简单机制不行的，怎么办？我们大家如果本科上过操作系统课，对吧？或者说上过相关的课就会知道啊，
09:45我们现在的这样的一个计算的网络，它是一个七层也好啊，六层也好，五层也好，无所谓，它实际上是一个 layer in 的架构。
09:53什么意思呢？就是我最底层啊，解决你的这个网络包传的对不对啊？这样一个问题，传输层对吧？传也解决的是我的点对点怎么传输啊，
10:03会有一个 neatork layer 啊，会去解决我这个包对吧？我比如说我我我的这个这个那么多十万台机器，我如果要传到这个十万台另一台这个十万台机器中的某一台，然后我跟这台机器又没有直接的对对连接怎么办？对吧？
10:15我得有个网络层去搞这个事儿，那后于网络层还不够，我的包可能会有乱序，对吧？然后我包如果出错怎么办办，我难道就直接丢掉嘛？
10:23所以我们会有一个这个 transport layer 会去做这个重传，对吧？那么在更更加之上应应用，就会根据自己的这个需求对吧？去做一些这样的一个定制 OK。那么这一套协议是就是从功能性上来说，
10:37它其实 work 的非常好。大家在这个 linux 啊操作永擎都会实现实现这一套。那大家有没有想过这一套呃 work 很好的 TCBIP 对吧？它这个会有什么问题呢？我们说所谓这套治疗系统，它要工作的一个前提是我们得依赖于这个一个实现，
10:56它把我们前面讲那么多协议栈都有效实现。那么你大家看站在我们用户的角度来说，我们是不 care 这些的，对不对？我们自己写网络程序的时候，我们其实就调调了一个这个这个这个这个这个这个这个网络库调个包对吧？
11:10或者列表调个函数就结束了。但是在我们说系统里面有很很经常说，就是我们要思考一下这个你调的这个包它背后发生什么样的一个过程，对吧？那比如说在这样的例子里边，我们用户如果调一个传统的这个 GCG 网络的这样的一个通信，
11:24比如我就要发一个消息，那么开机器好，我们先需要什么？先需要把这个数据拷贝到内核里，为什么？因为用户这个内核他可能就会改这个数据，
11:34对吧？这个这个这个他把它组包啊，我这个用户啊这个不希望呢不需要他不不会改用户的这样名义，然后呢我就就封装成 socker。然后最后的时候呢，哎我们又得过 TCPIP 协议，
11:46就就得加上这个包头，对吧？我们说网络包，我怎么判断这个网络包啥的，对不对？然后我得加加赞啊，
11:52一系列啊，组完包之后对吧？我再得调驱动，对吧？因为大家想想，我们本质上什么本质上我们需要网网络这个硬件把这个包发出去。
11:59这个东西其实是是写，不过网络驱动其实是很很很很苦恼的。你得去啊写合适的这个 MML 啊，写合适的呃编程这样的一个啊这个这个这个 driver 啊，这这个这个 redister 对你搞完之后呗。好，
12:13这个时候操作系统不用干啥事，他这个 driver 就会帮你把这个事儿弄上去。然后这套东西都被我们这个 poicx 里面的一个非常极简的这样的一个接口，包括这 soccket by 这个 listen，这些接口全封装好了。当然这个 soccit 有一些复杂的这种接口，
12:31比如说它一库对吧？那这种其实啊其实本原理上它只是说只只是还是前前的套，没有没什么特特别特别的改变 OK。好，那么大家想想我们这一套东西处理下来啊，我们为了支撑这样一个协议处理下来，
12:44它会有什么问题呢？它会有什么问题呢？其实一个核心的问题就是它的软件实现太复杂了。这个左边前面是我们把这个我们之前讲的那一套啊，非常简单的啊，非常啊以一个 high level 的方式去展现。
12:59但是实际上你在 linux 里面，它会要处理很多很多的这种框的 case，以及这样一个细节。包括啊我在发包的时候，这个时候这个网网卡如果它正在忙怎么办？对吧？
13:08我得做调度啊，我收到这个包的时候，我这个应用没有切回来怎么办？我得把它放到这个里面啊，我然后我得做即时 buffnotificficy，然后我得及时分配这个内存啊，
13:18一系列的这样一套东啊，我们就能看到这是钢筋的这个 linux 内啊，如果你把那个 kano 的这种各种模块放下来，就会都会就会会做，就这么复杂的这样的一个不层。好，
13:31那这边就会有一个问题，就是这么多层啊，看上去很复杂，对吧？它复杂性它其实并不是一个，你说这个东西很复杂，
13:38但这这这这并并不是个贬义词，对吧？复杂有什么呢？只要我能完成任务就行，那么复杂会带来什么样的问题呢？在我们这样的一个场景里面啊，
13:49其实我们做了一个 profile 啊，就能发现一个事情。就是这复杂杂的结果导致就是我在做一个网络啊，尤其在现代数据中心的这种网络里面啊，我去通信的这个物理开销反而不是很大。前面在就是我从物理上我的网卡把一个包发给你，
14:08运营包在现在的数据中心，你能做到这个一微微秒这样一个级别啊，规模不大的话，你能做到一微秒这样一个级别。然后我我比如说我处理一个正常业务逻辑，比如说我就处理一个这个 key by ststore，
14:21对吧？它其实也就能也也需要在这个其实也是在八微秒到到十微秒到一百微秒这样的几率就完成。但是啊我们为了比如说为了做分布系统，我们得发一个网络消息，让对面那个机器帮我做一个 PR 做请求。这个里面我的这个 linux 的 TCVIP 的这个 sack，
14:39就需要花五十到两百微秒这样的一个事情。那这种设情的话，在以前比如说十 GBPS 的这样的一个慢速的网络下，其实没有什么特别大的问题。但是现在的网速啊，我们这边提了一百 GPS，
14:55但是最新的这个给 AI 的这个网卡，对吧？已经飙到八百 GBPS 了啊，当然这八百 GPS 好像停了一年，就是它并没有延续到这个更更高啊。就是去年好像也最快的也是八百 GPS 啊，
15:09可能是遇到一些工艺的问题。但是我们可以明显看到你 CPU 其实也有工艺问题，对不对？所以这就意味着什么？意味着说我们对于传统的这个 linux 啊，做做几十年经典的沉淀下来的这样的一个网络。
15:23在面向这样一个高速的这个网络下，它就并不合适你的 CPU。你的内核啊，其实会成为这样的一个主要的这样的一个瓶颈。所以我们需要去解决这样的一个瓶颈，就运出出了很多的样的一个新新的这样一网络技术。
15:39大家可新东网络技术。比如说第一个演化出来做什么，也不能说第一个吧就是一个很经典的演化出来的一个方法叫做 DBDK 啊。 DBDK 呢它的思想其实很简单，就是说我既然我 linux 对吧，前面 profile 出来那么多的 overhead，
15:58其实在 linux 上，那我是不是把 linux 去掉就可以了。比如说我能不能让用户直接操作网卡，让网卡去发包，那这样的话肯定就没有 linux 这样的 overhead。那这个事情可不可以实现呢？
16:12其实是可以实现的。大家想想 linux 本质上他他这 linux 内部他怎么去操作网卡，它其实本质上它就是调用一个 driver 的这个驱动的这个 API，对吧？其实本质上就是叫 API，就调 driver 的那些 API。
16:26然后它可能会有一些这种复杂的这个 memory mac 的 IOR 这样的一个东西。那这些东西呢，其实我们如果把这个原本留给 driver 的这些内存映呃映射到这个用户态。那大家想想，理论上来说，是不是用户态也能够操作这样的一个驱动。
16:43这个其实就是前两年很火的一个东西，叫做什么叫 kneby？ ypass 的原理。原其实其实简单，就是就是我把原本在内核里的这些资源，我把它重新映射到用户态啊，
16:54不就不要搞这个原理上。很简单，它里面有很多很多问题啊问题，这个我们先不说好。那么具体来说，在 DBDK 这个 case，
17:01它干的事情其实就是把网卡网卡。其实大家想就是一个发送发送队列和一个接收队列，本质上它的硬硬件给我们这个驱动的这个抽象。那在 DBD 干的事情就是把这这网网的发送队列和接收队列映射到到用户。这这样我用户态就可以像在内核一样啊，操这样的一个这个设备了。
17:20当然这个近年来有也有挺多其他的设备，对吧？比如说你的这个这个 SPDK 大家有听过吗？就是你 SSD 操纵存储啊，也是这么设计的。本质上就是把什么硬件的这个驱动映射到用在好，
17:32这样带来的一个好处就是什么呢？我的用户态好，利用 DBK 这个库啊，我就直接可以把包物理包扔到扔到网卡上了，网卡把它发出去。然后我不是用做任何，
17:44不用经过 linx 蝌蚪，然后这样的话我的什么速度就能达到很快 OK。好，那么大家的这样带来的话，它其实就会有三个好处吧。第一个好处就是我没有进科诺的这个开销了啊。
17:58大家我不知道大家有没有测过，其实在现在的这个计算机上，你去进一次科诺的开销，实际上是非常非常大的。比如说你 CPU 进一次科诺的开销，大概是零点一微秒啊，
18:09到零点五微秒。如果你开一些防护的话，它可能会飙到一微秒。这个这么个这么个层层级。大家注意网速，网络传一个包也只要一微秒。
18:16然后如果你是 GPU 上，对吧？你要进一次科诺，比如 GPU，你你出来一个 page fault， GPU 也有 page foot，
18:23对不对？你出了 page fault 的话，它得它更慢。为什么它得它 GPU 上出一个 page fault，你得放到 CPU 上去处理。然后 GPU 和这个通信大概也在一到两微秒这样一个量级啊，
18:35额额的的这个通信它会比所以说老底还还不考虑进内核，对吧？所以说你这个这个进内核这个开销其实很大的。那么你有 DBTK，我就没有这个进内核的这样的一个开销了。那第二个好处就是我没有必要这个这个这个这个去这个这个这个这个这个做物单网络拷背对吧。
18:54我们直接说，你要让内核去帮你处理网络包，我得把什么包包先 call 内核核，然后内核再咵滚一遍再发出去。你 DBDK 我不用，我直接告诉网卡，
19:03网卡是我要传传着光，那网卡有一个东西叫 directmemory ory access 什么意思？就是我的这个不仅网卡，现在基本上 PCI 设备都有这个能力。就是我这个设备能够直接去读取你的 CPUU 内存的设置数据啊发出去。那这样的话，
19:17我 CPU 其实就可以 free 出来去干别的事。那它这样的话其实也带来的就是一个直接性能的好处。其实 DVD 大家想想它还有一个第三个好处，就是我们说你原本为了支持网络的这个路由啊，什么能力啊，容错啊能力，
19:31我其实需要构建这样一整套这个 TCBIP 的这样的一个协议站，对吧？我对这个协议站都滚一遍。但大家想想，其实有些应用我其实并不需要滚那么多的协议站。比如说呃我之前有有一个工作，
19:45对吧？一四年叫做叫做米卡。他就说我如果这个网网我我这个网络包啊，它只用来发 PPI 的的请求，然后我的 QS 的请求都特别小我，其实完全没有必要拆分包拆包。
19:56因为我发一个包，它本质上就是个原子的这样的一个这个有点强。然后它的这个网卡前后端，你做一些这样的一个容错，所以呢我就可以完全不用携战啊，这样的话我的性能就是非常的快。
20:07所以 DBDK 这种方法在实测上啊，它其实是能够啊很很很很低的延时，以及这个很高的这样的一个吞吐啊，把网卡的这个带宽都打满。但是 DBDK 它有两个非常关键的这样的一个问题。第一个问题就是它的这个接口啊，
20:26就是操作系统，它的这个抽象是和 TCPIP 完全不一样，也就意味着说什么呢？你如果原本一个应用，它是给 TCPIB 写的好，你现在用 DBDK 你得怎么办？
20:36你得用 DBDK 的这样的一个接口重新写一遍，所以它这个 comeltibility 上有一个问题。然后第二个最大的问题是当时的这个推出 DBDK 的时候啊，网卡它本身它是不提供重错，不提供乱序重传啊，乱序乱序乱序乱序重排。
20:55然后不提供这个重啊丢包重传这样一个能力的。所以如果你就是说你 DBDK，你让网卡发了个包，那这个包它有具体有没有发到对面，这个事情应用是不知道的，不知道的。
21:07大家想这个事情原本是谁谁干的，原本 TTCPPP 干好，我们现在这个是把 TCIP 去掉了，意味着什么？意味说这个能力它没有了，他还得什么还得用户去实现，
21:18除非你不 care 这样一个包，你也可以自己实现些简单的通杂。这就导致一个问题就是 DPTK 非常非常难用。当然有一些这个 solution，对吧？比如说 MTCP 啊，
21:29不知道大家没有听说过 MTCP 是应该是腾讯吧啊，就国内的某个公司提出了一个用户态的 TCP 的这样的实现啊，什么意思？就是我在用户态啊把科诺态重新实现了一遍。然后呢，我这个用户态 TTMTCP 可以接到这个这个这个这个 TPTK 上。
21:45那这样的话我要用 TPCK，我不就能用，我就又能 TTC 就就能想到这个 DBDK 了嘛。那但是这套它的问题是什么？他其实他其实问题是大家可以想象一下，就是他如果我们用了 MTCD 加上这个 DBDK 的方案，
22:01他牺牲了上面的哪三者呢？其实我们可以看到他其实只解决了什么？只解决了上面只保留了上面一个白 pass curo 的一个特性。它里边的这种协议的栈的处理开销，对吧？也好，
22:12内存开拷贝开销其实并没有消除，对吧？大家想想我我内核还有 inux 态，我这个 linux 内核开发流都非常牛逼，对吧？他们开发那么多年的一个网络芯站，
22:23我凭啥你你你一个第三方的来了一个 TCPIB，实现的比我内核实现的还好的。它们实际性能上其实差不了太多，差不了太多。所以导致的一个结果就是当我们用了 MTCB 加上这个 TPTA 的时候，大部分情况下这个这个 MTCB 本身它就是也是一个这样的一个这个瓶颈，
22:43它也是这样的一个瓶颈。所以说你在这个 DPTA 相当于啊就是啊用处就就就没有那么大啊，没有那么大。所以呢我们就意味着说我们需要更好的这样的一个方案。那么这个之后人们就在想的一个事情，就是我能不能把整个网络站全扔到硬件上，
22:59实现大家想硬件 of loading 是系统里面这个非常经典的这样的一个或者题结构里面非常经典优化控法，对吧？那想想看，我们这个软件执行的开销来源于什么？它本质上不就来源于我们要做一个计算。但这个计算它不能够直接表示成这个这个这个这个处理器。
23:18不管 CPU 也好， GPR 提供的基本的 ISA，就 v right 加减符号。所以我们得去做什么，做一系列的这种翻译，对吧？
23:26基本上这些翻译这些翻译是带来开销的那如果我可能告诉你我硬件里有一条指令，能够直接把你的这个这个功能做掉。那它的每个 cycle 的这样的一个算力释放其实是最大的。那么这边就会就引出了，就是 RDV 啊，它就是这样一套东西，
23:42它包含了一套这个完整的这个网络协议栈在硬件里实现。那我们在用户角度来说，它的用法和 DBDK 是一样的啊，它是一个科 en by pass 啊，它是没有 memory copy 的。然后呢，
23:55但它跟 DBKK 不一样的是它的网网卡它直接实现了这样的一个我们所说的丢包重返也好，这个这个抗这个乱序乱序乱序这个重啊，乱序重排啊，这个其实都都实现了啊，所以呢它既带来应享受了这个 DBK 的高性能，对吧？
24:14又又解决了它的一部分的这样一系的问题。但是 RDV 有个核心的点就大家是这个这个问题，包括也是我我之前一直很很困惑的问题。就是大家想想，我们之前说过有 TCPIP 这样的一套协议，软件时间开销大。
24:28那我们为什么不直接把 TCPIP 这个协议扔到硬件上去实现，而是重新去重启一套炉灶，对吧？是造一套 RDV 协议的。我们这个明确跟大家说 RDMA 它的这个底层的网络协议和 TCP 是非常不一样的啊，它包括它的重盘机制也好啊，
24:47重繁机制也好，都 TCPP 要简单很多，简单很多，为什么呢？直到前两年啊，我看到一篇 paper，
24:53我觉得它总结的其实非常到位。就是 TCPIP 啊，它本质上是一个这个这个从这个软件的角度来思考，怎么去解决这样的一个这个这个这个网络丢包啊、乱序啊这个这个这这些问题，然后包括用测控制啊，
25:09然后呢这套方法它其实并没有考虑硬件亲和性，就是它虽然方法上是很很好，但它硬件不一定能高效实现。那 RDMA 这套协议呢，它实际上是从硬件出发，就是它考虑的第一软件的基本功能，
25:22它得有。第二呢，它考虑的不是说你这这个软件到底用起来方不方便啊，并不是它考虑什么呢？就是说我这个硬件实现起来快不快啊，快不快。
25:31所以呢它导致这个结果就是它的网络协议其实会比计算机复杂要简的不简单一点，对吧？大家想简单，我才能够放到硬件上去实现 OK。那所以它其实这样的话才催生出了为什么就如说之者的这个这个协议会这么多。当然其实这个这个最近几年啊，
25:47我们观察到的一个趋势是是 RGB 的这个趋势也开始变得越来越复杂了。比如说之前那个就是呃狗白，不知道大家有没有听说过，就是就是 go go 拜。三是一个网络里面处理丢包重传的一个经典的方法。就是说比如说我我要发一个发个网络信，
26:05光，我发一个大的 message 对吧？然后比如说我零一二三呱呱呱叭，那么多个这样的一个 n 个消息。然后呢，我比如说我连发了一个包之后，
26:14我突然发现其中的一个包丢了怎么办？丢了怎么办？那 go back 和 n 呢，就是说我我要把后面的全扔掉，然后把这些全都重开，全都重传，
26:26这叫 go back 和 n 啊，大概是这样啊，这样方法大家想填有个很明显很明显的问题，就是我如果这个这个这个后面又传了很多包，我当中只丢了一个，那我不就会浪费了很多这个包嘛，
26:38对吧？那么这个显然不是一个很好的方法，但是在 TCBBP 不是这样样， TTCP 实实上就叫做一个叫 selective。 transfer，就是我丢了哪个我就 transfer 拿了，
26:47它肯定不会把这些全都冲出来 OK。那这个这个这个是 TCP 做法，但是最早 RDMA 它为了实现的方方便其其，并并有有实现 select transfer。它实现的国拜根啊带来的结果就是在某些 case 下，它的这个性能其实会下降了，
27:03非常非常多。那大家想想它 RDB 为什么不实现国拜 ke 根呢？为什么不实现 select transfer 呢？大家想这套东西我们得在什么？得在硬件上实现，得在硬件上实现意味着什么？
27:13因为说我们需要去 track 我一个理由里面，对吧？它到底有多少个包是空的，多少个包没空的那大家想想如果我是国拜根的话，我其实只要实现什么一个 counter 其实就可以了。所以说这套方法其实国拜根在这个硬件上的实现，
27:27其实是相比较这个 selective transfer 实际上是更加的这个这个高效的啊。所以说哎 RDA 当时当时啊去选择了胡外分这样的一个设计。当然这个最近两年我们说为什么呢？就是最近两年可能得益于这个工艺的进步，对吧？或者你这个制程越来越越显，
27:43以及人问对于这个网卡设计的技术越来越高，其实有一些很探索索就是说是说其 sesective transfer 其实在 RDMA 下也是可以实现的。就两者可能会有一个这种交流，但本质上来说，他们的协议还是得有一些区别。就 RDV 这个协议，
28:00它更加的这个对于这个网卡来说是更更加的这个实现上友好 OK。好，那么就是得益于这样一个 RDMA 的这个可辨的性，对吧？又得益于这个非常高的这个性能，对吧？
28:15所以就就催生出了一系列的这个 RDV 系统。当然现在几乎所有的这个 AI 系统都是跑在这个 RDV 上的啊，这种其实基本上都已经不用太多考虑了。在我我读博士的那个年代，对吧？你用了 RDV 系统还算是一个加分项。
28:28对吧那现在的话其实 RDV 已经是个标配了啊，大家已经觉得都都不需要不需要再提它了。 OK。好，那么 RD 杯它本身是一个这样的一个这个这个协议啊这样一个协议，然后或者说它是一个思想。
28:46但是你本身你要把这个系统啊，把这个网络对吧？用起来啊用起来它其实是是需要有一些具体的实现的。然后这些具体的实现到底该怎么实现？那其实现在啊还是这个这个有很多的这个争论在啊，就是说就是说或者说各种实现都有。
29:04比如说第一个我们实现叫做 efeit band 大。如果听说了，它实际上是 RDMA 协议的一种一种实现 ID 啊，它最早实际上是来自于这个超算中心啊，超算中心。然后超算中心的话，
29:20它的假设大家想想大家有没有想过一个事情，就是超算对吧？跟我们说的 cloud computing 到底有啥区别啊？它们其实是有很大的很大的区别的。我们在在在从从硬件上来看，不都是一堆这个机器嘛，
29:33不都是一个 class 的机器，对吧？那为什么说超算和和和和和云云场景数据中心是不一样的？然后然后我们说 IB 它实际上是面向超算设计，所以它的这个这个这个 RDA 性能实际上会比面向 cloud 设计的这个这个这这个这个 RM 稍稍微快点们们们大有没有过？为什么实本质的区别在于超算算个个这个这个这个平台啊，
29:54它面向的应用的访问模式是独占的。什么意思呢？就是说我一应用它用了你这个超算的硬件啊，这个硬件基本上就给你跑了。就比如说你跑一个大规模的这个这个物物理模拟，或者就跑一个大规模训练。
30:07你这这集群群不会会其他他人卸。但是 cloud computing 和这个超算最大的不同，就是我一个 coled 的这个 cast，它其实是给所有人去卸的啊，给所有人去卸卸。所以说它一定得做什么？
30:18做 perperformer isolation 得做这个 congest 更更复杂的这样的一个 congestion control，就用色控制。所以这导致的一个事情就是 RDMA 目前的协议其实主要是分成两个流派啊，第一个流派就是面向这个啊这超算或者说类超算使用场景的这样的一个 ID 这样一个协议。它其实我就就像我刚刚前面说的，就是说它假设的就是用户，
30:42他是不在这个集群使用。所以它的这个不管是流控也好，它的这个啊这个音色控制也好，做的其实会相对简单一点。但它的问题就是它的这个成本其实相对来说会更加高一点。那么所以呢为了让这个 RDA 更好的去对接到现在的这个数据中心网络，
30:58对吧？那么有有一些新的这样企业，比如说 locke 的这样一个 VV 一和 v 二啊，本质上就是什么？就是我在以太网上啊去构建一个这样的一个 RDA 的这样一个 transport。那它的好处就是说我第一，
31:11我跟现在数据中心的这个网络 infrastructure 是兼容的啊，我只需要去换一下网卡啊，就能够去支持样的一个这个跑在现有数据中心上啊，但它的问题就是你如果跑在这个以太网上，其实目前来看的都有很多的这个网络上的问题需要解决，尤其是啊规模变大啊，
31:27规模变大的这个时候。因为我们前面说你这个原本按里面假设的是啊我的这个独占的这个使用，当你把它变成卸的时候，其实会就会有一些问题包。那具体的大家可以去看相关的配备啊，这个非常配备非常多。
31:40当然还有一些这个这个这个这个协议是希望能够把 RDM 用到广域网，就是我能够跨数中心，比如我手机是不是也没有 RDV。其实最近啊我我看到一些研究，也也有在在讨论讨论这样一个事情。因为现在不是各种 AIPC 吧，
31:54对吧？ AIPC 这种小盒子算力盒子越来越多。那么大家也希望这些盒子能够联动起来，做一些通信。那你现在这种手机它肯定没有 RDV 这种能力，所以说它的有 OOI，
32:05所以也其实也会有有些人在看怎么把案例应用到这个小盒子上 OK。好，所以说我们可以看到的是啊，不不管你这个用在哪，对吧？其实这其实对于这样的一个网络方法啊，
32:17它在这个这个这个各种使用场景下，其实在目前来看还是非常有用的。尤其是你的算力 CPU 算力成为这个瓶颈，网络又变得越来越快啊，这样的一个一个场景之后。好，
32:29那我们接下来就带大家看一下，如果我站在一个应用的系统的开发人员，用这个开发者员角来说，我们怎么去用啊， RDV 啊，其实我们看完之后，
32:38我们其实就能够感觉到为什么说哎它这个这套新的接口对吧？对于这个这个这个网络硬件实现相对来说是非常友好的啊，这样一套它跟我们之前呃编程这个 GCBID 其实都差的挺多的。 OK OK。第一我们说你要做网络通信对吧？第一个需要这个解决事情，
32:59就是说你的这个通信总得有连接，对吧？我们说 TCPIP，你得听这个连接，我才能发发消息，对吧？
33:06那 IDMV 里面硬件连接，但是它的连接呢跟跟 TC 在长什么样，它叫什么叫 QP 啊， QP 什么叫 QP 的。它这个本这个其实从这里开始，它就已经把这个什么网卡的一个稍微底层一点的这个硬件结构啊暴露给用户了。
33:22大家想想我们网络发发收发消息，它本质上什么？本质上就是个队列，对不对？本质就是个队列。然后 RDAA 的这个 QP，
33:30它就是把网卡的这个收发队列直接暴露给用户啊，它具体分成三个啊，三个就是就是三 q 啊，就是你要发消息，你就得往这个这个 q 里扔东西啊，还有个 receipq q 啊，
33:43就我要收消息，或者是我看这个之前那个消息有没有完成啊，我就往这个 q 里看。那还有一个 receicit q 啊，就是我如果如果如果收到了别人的消息，就把那个 receipq receicit q 里边的一个值 OK。
33:55然后用户需要干的事情，就是我把这个具体的 r 那 v 请求啊发扔到这个消息的这个队列里面。这个请求里面可以包括呃文三列的请求和 two 三的请求啊具区别。我们待会会会会具体说 OK 好，然后不同的 QP 可能会有一些不同的这个这个 model 啊，就跟其实跟 TCPIP 的这个 socket 一样的。
34:18我们说 socket 你有这个 TCP 的 socket，有这个 UDP 的 socket。那对于 RDA 其实一样啊，它有类似于 q es TCPIP 的这个 reliable 的 connected 的这样一个啊 socket 啊，它其实就是提供了 reability 和这个这个这个顺序的这样的一个保障啊，也有这个不可靠的这个这个 unreable data form。
34:36它其实跟 UDP 很像，其实在现代的这个 RDMA 的这个这个这个这个实现下一个这个 UDR 发的包，其实就是 DBD 可以发的包啊，它背后背后实际际上是共用啊，接近同一套实现 OK。然后还有一些这个更高级的这个这个这个玩法啊，
34:52比如说你这个 QT 可以啊可以可以跟多个人连接啊，但是这个的话就是还是比较是比较新的啊，我们就给大家介绍 OK。好，那么人们用的最多的一般来说是这个 reliable 的这样一个 convenicence，这样的一个这个 QPOK 就是提供可靠性可靠性。
35:10好，那么那么我们就来看一看，就假设啊我有一个这个这个 RDA，对吧？就是我希望能用 RDA 去去做一次网络通信啊，它跟传统的有什么不同，
35:22有什么不同？ OK 首先在 RDA，你要用这个 reliable connected 这样的一个 transport 去做通信的时候呢，你先要去建立一个配对，什么意思呢？就是我一个发送端啊，
35:32它有假设有三 q receicipe q 和 competiq q 它一定要跟接收端的一个三 q receicipe q 和 competiq 做一个这个配对啊，所以它为什么很多时候 RDV 叫 q 片，对吧？我个人理解这个有两层含义，一层是啊我这个本身我这个用户这呃站在发送者这边，他有一个三 DQ 或者 CDQ。
35:50然后站在这个这个这个整体的角度来说，我的这个发送方和和接收方得有一个这个配对啊，对吧？这个所以说它需要有个配，那为什么需要有个配对呢？大家实际上跟你 TCBIP 握手其实一样的，
36:01我为什么 TCBI 必须要握手啊？就是我我每一个信消息，它其实对应的是一个 session，对吧？这个 session 我需要去预留足够的这样的一个资源去去处理这个这的信息。比如说去记录哪些包来，
36:13哪些包没来，哪些包乱序，哪些包不乱序。这 RDV 其实一样，我需要有这样一个配对啊，在网卡册它其实就会去分配这样的一个队列的这样的一个资源啊，
36:24去跟你的这个发送册的这个人去记录这个发送的这样一个信息。好，那假设啊我们这个配置好了之后啊，我们其实就可以用 REB 通信了。比如说假设我左边这个人这个人叫 bob 啊，他想发一个消息给这个 alice，
36:41那怎么做呢？其实我们干的事情就是什么？就是把我们要发的这个消息啊给扔到这样的一个这个这个这个呃这个发作息。那这边有一个问题，就是在 pobob 想要发发消息前， alali d 这边要求我 bob 先去这个这个哦不 alice，
36:59我先要发送一个这种 post receive 的这样一个请求。这个请求告诉什么呢？他告诉说， OK，我将要收到一个消息，然后这个消息需要存在这个这个这个这个这个 post receive 对应这个 mssige buffy。
37:14但是这个设计其实就跟我们 TCPIP 不大一样了。大家想 TCPIP 是什么？我那个叫什么我收包这个事情其实我不需要预先提供这个网络包的呃网络 buffer 的对吧？它来了一个包之后，我的协议自动会帮你去分配这个法，去去做一个缓存。
37:29但在 bufr d 杯不是 RD 杯里，如果要做一个收益消息，你提前得先把这个注册好。那像这个设计它原因是什么？这个设计的原因是在于它可以节省很多的这个网卡的逻辑。那像 RDA 的协议，
37:44它跟我们的这个 CPP 不一样，它这个所有的协议是在硬件里实现的那从硬件的实现角度来说，你的这个操作越简单，对吧？硬件其实是越去越容易去实现。那所以对 RD 杯其实也是一样，
37:55它有了这样的一个这个设计，你得啊先做这个 post receipt。好， post receive。做完之后，那么站在 bob 的角度来说，
38:05我才能够发送 send。如果我这个没有 post receipt，我直接 send，那这个消息就就就收不到了啊，就收不到，就就直接会报错 OK。
38:13那 bob 这边发送消息，他用了一个 RDV 接口，叫做这个 host set。它其实本质上干的事情就是把这个 bob 这个消息啊对应的这个这个这个这个这个这个这个这个消息里面有一个这个这个这个 hello，对吧？它是一个地址，
38:28然后这个地址对应了这个 OP，就是告诉你这个网卡他要做什么事。然后如果是个 sam 的消息的话，那网卡干的事就是把这个 sam q 的这个消息啊，通过网络发送到这个 alice 这边的这样的一个这个这个这个这个 receive q 就就是 alice 之前注册好 OK。然后这这就是一个完整的这个流程。
38:51那么站在这个啊在 bob 这个完整的的流，那那么站在 sab b 这完度来的话话，它他要干一件事，就是我得知道这个消息什么时候做完，对吧？因为如果你很多时候你如果尤其是上我上节课，
39:03我们讲的这个网络协议，对吧？其实你你得有时候得收到收呃 python，对吧？你得你把这个消息收到收到，我才能算做完。
39:11那 bog 可以怎么做？它可以去这个这个就 put 这个 communq。就是说如果这个网卡它把这个这个这个消息成功的发到了 SCVQ 里，然后呢，网卡会产生一个这个 ACK 啊，放 ACK 放我这个 MVQ 里。
39:25这样的话我 bog 就可以通过这个啊库这样的一个 CQ 啊，来去判断它自己的这个操作有没有完成啊，它其实是有这样一个去 d back 的这样的一个机制的 OK。然后呢，站在 x 的角度来说，它其实是一样的一样的。
39:40那那么它其实会会会什么？它是会这个这个这个通过铺自己的这个 recipe q 啊，来去收到 back bob 的这个消息。比如说在这个例子里面啊，如果它碰到一个消息啊，他就 print 的出来就是这个 bob 发的这样一套 OK，
39:53就是这样一套流程。就是这这个在 RDV 里面叫做 q 三变的 RDV 啊，它流程其实并并没有特别的复杂。但是它但是它没有这个我们之前讲的这个 TCVIB 里面这种啊什么 listen 啊，什么其实都没有啊，他就是说我破破碎了，
40:07 c 不离收。 ok 好，这样一套流程。我们说实际上是对硬件非常非常友好的，非常为什么呢？我们可以来具体看一下啊这个这个这个这个这个 IKV，
40:19对吧？他发的这个整个这个消息的这个这个这个流程，就大家其实其实很很好奇，对吧？为什么我调了一个这个这个 post sam 这玩意儿这个东西它就能够对吧？很很顺利的哎，
40:34发送到了这个这个 alice 的这样的一个机器呢，它背后其实用到了什么？用到的就是啊 CPU 和网卡之间提供的这样一 eprctive 叫做 PCIE。我们说 CPU 和这个网卡它它的交方式其实就无啊呃系主流的以以 PCIE 为主。阵网啊，交互方式无非就两个。
40:53第一个就是 MMIO，就是说我去相当于告诉发告诉一个设备说，我我我往设设备上写个信号，告诉你这个设备我要做东西了。这个 MMIO 可以是不啊，你根据不同的这个任务的这个类别，
41:05对吧？你可以做这样的一个这个区分，然后设备干的事情。就比如说我们在这个 case 里面啊，我 IS 先 pos set，其实本质上就是我的这个阿里面的驱动啊，
41:14它掉了一个 MMIO 啊，告诉我网卡设备上的一个一个，然后网卡就知道好有个消息息要读了。那这个时候网卡呢它就会用一个 redisisd MA，我们叫 direct very exaccess 啊，就是说它我们前面提到过，
41:27那直接会绕过 CPU，直接就把这个数据啊，我们刚 pocent 这个 message 读到这个网卡内，然后做到读到这个网卡内的网卡，它就会有一套自己的硬件实现的这个协议栈。它就会把你这个消息啊拆包组包，
41:39拆包组包组成这个合适的这个 RDA 的包。弄完之后呢，哎他就会通过网络去发出去。而对面的这样的一个网卡。收到这个消息之后呢，他就会看一下，
41:50对吧？当前这个 QP 有没有这个注就 post 就就没没有注册一个收消息地址。如果有好，他就用 receiced MA 把这个收到的这个东西啊，包之之啊解包之后后到这个 DMA 内内存里。然后呢，
42:02最后大家想想我们这个 alice 怎么知道这样的一个这个消息息。成成我们是通过库一个 CQ，对吧？那么大家想 CQ 本质上什么队列，本质上什么它不就是个内存的数据嘛。那既然它是个内存的数据，
42:14我这个网卡也可以改，对吧？那网卡它写完之后它是什么？它就会写一个 flag，告诉这个写写一个 CQ，就在告诉这个 CPU。
42:22好，我这个东西做完了。然后这个时候 alice 如果在这边调一直调轮巡轮询，它就会轮询到哎，你的消悄它就做完了 OK。所以我们可以看到的是，
42:31我们如果不考虑这个拆包解包的话，这个它的这个其他的这个逻辑对吧？实际上是非常非常非常简单的。所以说它其实啊至少是在这个这个实验测，它其实它会比 TCPID 简来很多。他们有这种各种的这种这种那个叫什么这个这种分配啊，
42:48动态分配这些东西其实其实都没有啊。所以说为什么说 RDV 它能在硬件高校实现的原因？当然其实现在啊也也也有一些 TCPIP 的这个 of loading 的这个方法。但是它们一般 of 的都是就是说裁剪的 loadt， CPIP 不是一个完整版本的 TCBOK。好，
43:04我们就简单可以比较一下的话，我们就发现这个不不 DBDA 呃 RDB 对吧？相比较这个 DBDA 啊，它其实是就是一个这个个这个啊这个这这个这个这个相比来说是一个更加优的这个版本啊，比如说这个这个这个他们两个都是科本版的的 parr。 DMA 的话，
43:22它其实是有有这个完整的网络 stack，但 DBDK 是没没有的啊。 RDMA 的话其实是完整的这个 ZO cop 其实 DMA 来说不能算是 ZO cop。因为你为什么？因为一旦你要加一些协议的信息，你还是得自己去组包图。
43:36这些东西其实还是有靠背的 RDM，这不会，它都是 DMA，然后硬件直接帮你，你它做掉了。当然我要强调一下的时候，
43:44包括刚号我们也看到，就是说这个这个并不是说 DBDK 完全比不过 RDMA。为什么大家想想我 RDMA 这个硬件上帮你实现了这些协议？但是这个硬件实现的这个协议，它一定会高效吗？其实我们后面啊再稍微展开一些，
44:01它这个更加低效的实现的时候，我们就会发现啊，它这个有些时候它的硬件协议在有些情况下，其其并不是是高效。如果你软件层不不注意啊，这样的一种它不高效的这种啊访问方式的话，
44:13我们会发现你它虽然即使硬件硬件卸载啊，其实你也没有办法那个获得很高加速，而且更加大的问题。就是 RDV 本身这个硬件它是写死的。就是你你只要出厂了，好那个叫什么，
44:25基本上这个就很难改。虽然最近几年也有些可编程的 RDV 啊，那基本上来说是很难改。那这样导致的啊一个一个结果，就是我我如果用用法不当啊，基本上来说我也很难通过改网卡去去去把它弄。
44:38对啊，只能从这个软件层去开箱 OK。好，那么到这边为止的话，这个这个这个这这个我们就就可以先看一个简单对比吧。就是 RDV 和 TCP 在同等价格上，
44:51其实我们就会发现这个啊现在的这种啊数据中心的这种 RDV 网卡，它的这个整体的这个吞吐啊延迟啊，其实已经会比这个一个优化过后的这样的一个这个 TCP 其实高非常很多。其实我们可以看到这个 TCBIP 它是性的，其实是实非常差。 OK 好好，
45:12那么我们前面啊说了这个这个这个这个这个这个这个讲完了，就是 RDV 主要的这个数据面对吧。就是说我的怎么用 RDV 去通信，但我们这个数据面它依赖于一个很强的一个假设，是什么呢？就是说啊我的这个这个这个这个这个 QP 得配对上 OK。
45:30那这个时候那有个问题，就是我怎么去配对 RDMA 的这个 QP 啊，这个其实是我觉得一开始用 RDV 的这个人，不管是开发人也也好，还是我我我当时啊都非常困惑的一点啊。因为其实 RD 杯官方啊，
45:44他他他其实并没有去定义这个这个 RD 杯的 q 币，怎么对配对。就理论上来说，你你你其实可以用任意的这个方式，或者你只要用任意的方式配，只要什么呢？
45:56只要你能把这个配对的这个原数据给合理合适的呃，传到这样的一个唉就传到这样的对面。什么意思呢？就是我们我们可以来看一下啊，就是 RD 杯啊，它这个配对的这个原理本质上什么？
46:09本质上是要把这个发送方的这个对信息以及他的那个地址啊去去传递给接收方啊，使得接收方知道 OK 我要发这这个消息得往哪个队列走。而不是说这个这个这个这个这个这个任意的可以可以做这样的一个这样的一个匹配。但问题来了，你这个信息它是怎么传递的呢？因为我这有点类似于这个 boost shop 的问题，
46:30对吧？就是我我们要干的事情是我们希望发送方和接收方去做一个这个通信。但是为了做这个通信之前，我们先要有一种通信方法，把这个发送方这个消息啊给给接收方。那这个事情怎么做呢？
46:45其实 RDMA 啊它它它的官方其实并没有并没有说明。所以现有的这种做法大家可以看到，就是很多用 RDA 系统啊，它还是会里面会有一一些 TCP 通信。然后它官方方例的代码，其实也是用这个 soccy 的去做连接的。
47:00什么意思呢？就是我要先用 RDV 啊，我要做 RDA 通信之前啊，我先要找一个这个另外的这样的一个网络 OK 这个网络，把这个 RDA 这个原数据先在两个节点之间做一个同步。同步完之后，
47:15我的这个接收方和发送方对吧？之样才能用阿里云通信。好，我们具体可以来看这样的一个例子。所以我们可以看到就是说这个这个 RDV 的这个整个建建立链接流程，实际上是比 GCBIP 更加复杂。
47:28 CCBIP，我直接 connect 一下就行了啊。 RDV 不行，你得你写一套完整的这样的一个啊使这个建立连接方式。具体来说啊，就比如说我一个发送方，
47:37对吧？想要去和一个接收方去通信。然后呢我先首先先在网卡上去创创建对应的队列啊，它有对应的驱动的 AGI 包 x 可以去做。然后做完之后呢，我们要用过一个叫 q 瑞 QP 这样的一个 API，
47:51把这个我的这个队列的这个信息啊给给给给拿出来。然后拿出来之后呢，我要把这个信息通过另一层网络，注意是另一层网络，不是 RDV 啊，当然其实也可以用 RDMARDMA 里面有些很隐晦的这个接口可以让你去去调用。
48:06但是基本上这接口都我我我我自己当时看的时候，我看都没都没是文档啊，基本上都是在它的这个驱动代码里找到啊，有一些有一些有。但是但是大部分来说，你这个用起来很方便啊，
48:17比较常见的。可能还就是说我就用一个这个 TCI 啊或者是 UDP 啊传到接收方啊，接收方呢他做完这个之后，他就啊也去创建自己的这样的一个 q 当然这这 QQB 可可以去不创创建啊，为什么？如果你已经有个创建好的，
48:31对吧？我就可以复啊啊，因为 QP 是有一些这个复用的这个能力的。好，当我创建好之后呢，我就用这个 QQB 的信息啊，
48:39他去把这个 QP 给相来。这时候有点类似似于这个 CC 的这个握握啊啊把它 confict 成一个能用的状态。这个时候呢有意思啊，这个时候他的握手其实会有通信啊，它其实会网卡之间会自己做一次握手啊，这但这个用户是不用 care 的。
48:53我直接摸着台 KP 时候他们啊会做一次这个握手。然后弄完之后呢，我们会把这个网卡会把这个这个这个信息啊我们再返回过来。然后我在发送完同时也要再做一个类似的这样一个握手的东西。然后最后最后啊当当把这两步做完的时候，我的这个这个发送方的这个 QP 啊，
49:15就能够这个这个和接收方的啊这样的一个 QP 啊，去做这样的一个这个这个这个网络网络东西了。所以我们可以看到的就是 RDV 这个东西，对吧？它虽然在数据面，它其实我觉得是挺简单的啊。
49:27但是如果你要把它做一些控制面的这个这个啊，其实你要把它这个网络连接建立起来的话，其实是其是需要写一些烧子的代码的。我们啊其实我们当时写阿 d 就说很多代码其实都是都是在这个搞这个这个控制面对吧，怎么写啊？这个东其实相对来说比较还是比较比较比较比较 tricky 一点 OKOK。
49:47好，那么当然这个其实我觉得原因也是来自于它的这个四 s 天其实并没有讲清楚，对吧？他到底有没有什么什么特别好的方法？它其实啊你有有很多种方法都可以实现它 OK。好，
49:58那么到这边为止呢，我们来简单的总结一下啊，我们介绍一下这个 RDA 和 PPT 的基本原理。好吧？行，那我们呃，
50:06先休息会啊，嘘是你嘘。