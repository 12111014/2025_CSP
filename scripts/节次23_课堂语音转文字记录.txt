00:01是嗯是七是嘘是是啊嗯嗯，啊我们继续 OK 上节课，我们简单介绍了对吧？为什么说啊我们经典的一开始大家教科市小学的这个 DVIP，对吧？ work，
05:41然后我们介绍了这个 RDV 和 DBDK 的一些基本的原理，对吧？然后以及 RDV 这个收发消息啊，这个事情能不能 work，能不能 work。然后好，
05:53我们接着继续看我们上节课。最后收尾的时候，其实讲了这个 RDV 的一个这个用收发消息的做了一个事情。然后大家想想这样的一个用 CPU 去发消息的这个 pattern，对吧？在现在的这个很多的这个计算场景下，
06:08它能不能啊能不能很有效的工作呢？这个时候人们发现其实啊有些场景还是有些问题，什么时候什么意思呢？大家看我们发现了一个就是或者说我们可以观察到的一个趋势，包括这个趋势。现在其实也还在有，
06:20就是最近稍微缓一点，就这个图啊，它是这个 CPU， fragency 就是我 CPU 的这个算力，对比这个网卡的这个带宽的这个趋势。我们可以看到的一点就是在二零一零年之后，
06:33对吧？ CPU 的这个算力几乎是停滞了，这个其实比较好理解，对吧？摩尔定律这个停停止了停止，停停停止不行啊，
06:40但是这个网卡的这个带宽一直在涨，而且这个带宽涨的这个趋势，我可以这个图网然只画到二零二二零。但是我可以告诉大家，是至少在二零二五年这个趋势还是还存在的。二零二六年不一定，
06:53因为二零二五和二六我们目前观察到这个老黄家的最新最新的网卡都停在了八百 GGPS。但是我们其实可以看到的是，这个至少到目前为止这样的一个常速的增速还是非常快。那么大家是不是好奇，为什么这个这个这个这个网速能增那么快，对吧？
07:10 CPU 为什么没有增那么快？其实大家想想网卡它做的事情和 CPU 做的事情有一个非常大的区别，就是网卡做的事情它是非常容易 parallize 的。什么意思呢？就是我处理一个网络包对吧？那这个逻辑基本上跟你处理另一个网络包的逻辑是结构开的这两个完全是可以并行并行输去的。
07:29但 CPU 不一样，但是像 CPU，现在它有一个这个这个有 cash coherence rence 吧吧是有这个数据之间的同步啊，你要保证这个 coherence 这样的特性，所以它会有一些心心的的这些东西，在门的网卡没有没有没有没有没有没有这样一个。
07:44好，那么这个一旦这个能 paris 的这样的一个东西，它会带来这种好处来，就是我能够堆硬件去去去去给他增速。什么意思？大家想现在包括现在 GPU 也是啊 GPU 本质上来说，
07:56它对 cocs 的需求是是靠是让用户去处理的那这样的话大家想我们现在的最新的 GPU，它是怎么通过提供更大的算力的？在我单芯片的这个算力很难切上去，它其实就胶水就是我把两个 GPU 拼在一起，对吧？包括华为的这个 NPU 其实也是这么难，
08:12对吧？九二零 c 的这个型号，它基本上就是把两个九二零 b 给拼在一起。那么我既然有两个 CPU，它其实两个 GPU 它其实就就就更更好啊，它显然能更好的这这个吞吐就会翻倍。
08:23那么这个网卡其实也是一样的。那么网卡的话，它其实就这的本质，核心在于说这个它执行的是一个这个 parents for 效用，就是我能够通过粘胶水去把它粘在一起。因为你 CPU 把两个 CPU 粘在一起，
08:35它不是那么简单的一个事情。你两个 CPU 粘在一起，其实站在编程人员角度来说，它其实还是两个 CPU，它是两台机器。但是你网卡你把它粘在一起，
08:42其实我们是很容易把它一起，其就只有一张网卡的。当然最近最近可能呃这个粘也不是也没有那么容易啊，但是那至少这个粘这个东西肯定是比你 CPU 这个两个粘在一起啊容易很多的。好，这个时候我们就会发现一个事情，
08:55就是啊我 CPU 的这个速度啊有点停滞不前了。 OK，但是网卡没没事啊，网卡我其实速度在越来越快，其实现现在现在来看还会更快啊。因为现在都有一些这种池化的网卡的这个这种架构，
09:08就是我一个机架啊。我一台机器网网卡不够怎么办？没事，我可以再给你加加更多的网卡啊，现在都是这种这种硬件都开始出现。就导致这个问题就是我们原本的这个计算的这个范式啊是 CPU 驱动的，
09:21什么意思啊？就是我的这个要收花包，对吧？我都得 CPU 做。 ok 在这个时候我们就会发现一个问题，就是 CPU 它的这个吞吐会成为这样的一个瓶颈啊，
09:33 CPU 的这个吞吐会成为这样的瓶颈。那这个时候我们来怎么来解决呢？当然这个这个这个这个从吞吐的角度来说啊， CPU 如果你的算力就是不够发这样的一个这个这个网络包，就你不能发足够的网络请求，让这个网卡满载的话，
09:49这个事情其实是搞不定的那现在的人们的做法什么呢？比如说你刚才这样这样 deep ick 对吧？或者说他们的搞法，其实就是说我让 GPU 发网络包原理其实跟这个一样，就是它把 RDMA 这个驱动这套东西啊放到了这个 GPU 上去实现 GPU 它其实有一点特别大，就 GPU 它是它虽然是一个这个这个并行加速器嘛，
10:07但它的这个接口其实还是啊兼顾一些通用性的这跟 NPU 稍微不大一样。所以说它其实能实现一些 RDA 的这个协议的啊，那么这样是能解决通用性。但是这个 CPU 啊，你其实就就有时候就就很难搞啊，比如说我这 CPU 光收消息，
10:21它可能就收不过来 OK 啊，这个时候大家可能也会说一点，就是说那有时候你的这个这个这个 CPU 的网络带宽其实其实不是那么重要。就是有时候我虽然网络带宽八百 GPS，对吧？但时候我我其实大部分情况下，
10:35我其实并不需要那么高老老来回。尤其是你在这种互联网场景，就是说我们说的这种网网站啊啊网网不不是就那个那个就我们上上课讲的那个数据库这样一个场景，对吧？它加一下你作为一个数据库，它其实网络通信的这个这个这个传输数据量其实是很少的那在这种情况下是不是没有打满带宽也没有关系的啊，
10:55其实那对大家都对想想，对于这种场景的话，它的核心点什么核心点是要做一个低延迟，对吧？就是说我的这个消息能不能非常快的处理。就之前 AWS 做过一个这个统计啊，
11:09就是说一旦你的这个加购物车这个东西每点延迟高了一毫秒啊，他就会损失一百万美金啊。他他当时做过这样这样的损失，因为用户他可能等不及，他就他就不下单了。好，
11:19那么这个时候大家想想我 RDV，能不能就是我有 CPU 参与，这种情况下能不能达到一个非常低的延迟呢？目前来看对吧？其实是可以的。比如说这个这个这我们们当当的测略，
11:30如如果用我我们刚刚前面讲了一套 RDV 的机制去做一个收发消息的话，它其实能够做到二点二微秒的这样的一个延迟。这个延迟其实非常因为我们啊我们大概测了一下网卡的这个硬物理延迟啊，大概也就一一点多微秒，相当于你这个额外的开销也就一微秒，完全是可以接受的那这样的。
11:47但这样的一个代价啊，这样 r 键杯它如果你要用 CPU 实现一个低延迟，有个非常大的代价，就是你得用一种模式叫做铺利。如果大家呃做过这个网络或者设备 IO 的编程啊，就应该会这个应该会理解这样的一个概念。
12:03说什么叫铺利呢？就是说我要看一个，比如我要收消息，对吧？我要看这个消息来不来，我怎么做呢？
12:09我就是一直去去去去看这个 flag，有没有有个质啊，就是从代码上啊。但是这个就这个就是查那个那个一般阿里杯代码，包括 GGPU 实验器这样样，就是说我我就就就就是就去查，
12:22这就一直反复的去我要求去去看这个 flag，看这个东西有没有做完好。这样的话它确实是能够带来一个很低解，是为什么？大家啊我一旦这个包啊网网卡收到，网卡收到之之后，
12:33把你的这个这个这个这个这个类似于一写好，我的这个 CPU，其实就能够多退返，那我就能做了 OK。那大家有没有想过 pulling 啊，对于这个 GPU 计算很好，
12:43对吧？但是对于这个这个 CPU 来说，你做一些互联网用就有一个问题。因为 pulling 它意味着什么？意味着你的这个 CPU 一定是得处于一个这个满载的状态，就跑满的这种状态。
12:54也就是说你的这个 CPU，它的这个基本上啊你就得跑满一百瓦，就是直接放在那边。但是我们这网卡的功耗其实是非常非常低的啊，只有三瓦，这意味着什么？
13:06意味着说你一旦啊用了这种啊这种铺利的方式去实现一个低延迟，就会导致一个结果。就是你的这个服务器啊，它的能耗其实会会非常急剧的这样一个状况。大家注意就是说我基本上这服务器它其实大部分都不是满载的。因为你没有任务的话，
13:22它其实属于空闲状态，只有你能用多少种才才是满载。但是我们说你为了实现这样一个 RDA 的这样一个延迟，对吧？我们的 CPU 得处于这种处理的状态，导致这个结果就是我们的这个每一个服务器，
13:35它对于能耗，包括散热的这个要求都会变高。那这些东西作为一个系统而言，它其实是没有那么那么脆弱。比如说呃这边是我们今天统统计，也不是我们这接来就是去美国政府，
13:47对吧？它统统计一下，说啊你这个这个这个这个整个数据中心对吧？大量的这个能耗其实花在这个服务器上，以及这个服务器配套怎么冷却就是比较温。因为你如果温度啊，
13:58如果服务器过载的话，温度一旦升高，你就整个服务器不管服务器就就不 work。这一套需要一套非常复杂的这样的一个系统去指针去指针导致一个结果。就是如果我们想要用这种 RDM 的这种这种这种这种这种 pulling 的方式去实现延迟。我们的这整个服务器的利用率就会变得非常非常高啊，
14:17然后这个这个整体的维护就会变得非常非常困难。那么有没有一种方法能解决它呢？传统的这个这个方法就是传统大家做网络对吧？我们很很少会用 puling，我们怎么做的。一般我们用那种方法叫 interinterrap。
14:30就是说呃我的这个假设没有消息来，对吧？我这个 CPU 就就不不去铺了，铺了我就怎么我就把它 EO 给磕了，就让磕了做。然后呢，
14:40内核如果它收到了一个这样的一个消息， OK 它会触发 interrupt 你调用这个用户这个注册的这样 andandle。然后告诉你，你这个消息收到了。好，这个时候用户在处理它就相当于这个非常高高能耗和高能效的这样一种这个收消息的手段。
14:56因为你如果没有消息的话，那硬件它就不会触发 interrut。那你这个 CPU 它就不会跑，按照这个这个能耗就求非常低。我们可以很明显的看到，当你用这个 RMA 的 interrupt 去收消息的时候，
15:06它的这个 CPU 占用其实是比这个你用这个 pulling 的这个方式对吧？去做这个收消息啊，占用非常低的。但是 interrupt 有一个非常致命的这个缺陷，就是 interrupt 呢它得先掉到科蚪里，为这是内核提供的这样的一个能力。
15:23然后一旦掉到内核里，大家想想我们前面提到的 RDMA 的这种科诺 by plus 也好，是不是这一套东西它就失效了，对吧？就就相当被没效了。导致的一个结果就是我们我们之前测过测过过，
15:36就是一旦你用 RDMD 这个 interrut 好，一旦你的这个消息的频率比较低，你的这个延迟就就会从秒微秒飙到这个两毫秒。两毫秒。这个其实是非常夸张的。如果考虑到我们之前的这个亚马逊的这样一个统计的啊，
15:50我们会发现啊你的这个几几百万美金对吧？就没了。所以我们一个核心的诉求就是说我能不能有一种方法，对对吧？我能不够去节省这样一个服务器收息消息去发送消息。你你是你是逃不的，
16:05但我能不能去节省服务器接收消息的这样的一个 CPU 能耗。这样使得能使得说我能我能做一个能耗更低的哎这样的一个访问方式。同时啊我还能够去闹这个带宽的这个问题，对吧？获得这个缓解的这个就提到就是说 RDMA 就是一个核心的事情，就是说我们需要什么？
16:24我们叫 CPU。 by passing，就是我们在做网络操作的，其实本质上是我说的，就是我做做一个操作的时候，我能不能不要 CPU 我能够让硬件来做硬件来做。
16:35那这个时候呢 RDA 它最早啊其实就发现不仅是 RDM 啊，包括你现在这个做做大部分模的计算也好，发现很多事情我其实并不需要这个 RDA 啊，不不是 CPU 来处理。比如说我很多时候这个这个这个这个这个要需求，这就我去读一下这个远端 server 里的一些数据啊，
16:53这些数据它其实本质上什么就是一个这样的一个内存访问。就我 server a 要读 server b 这个数据这个操作啊，我们很多人发现它就是之前前最最早起的 RDV 设计的那发现这个东西它它很常见，然后它又它又它有个很好很好的曲线。所以他把这个能力硬化在了这样的一个网卡里面，叫做文塞给的 RDV 啊，
17:13就是说这个 RDV 就这样一个原语，它能够直接去读 server 的这样的一个这个这个这个这个内存的这样的一个数据。而不需要我 server 去注册一个这样的一个这个有 thread 在那边收消息啊，收到消息才叫 RPC 的方式做 OK。那这样的话它其实就能够不管是从带宽上啊，它能达到比驱 c 列的更好的这样的一个性能技能。
17:38同时它在能耗上其实也也会更加的好。就比如说这个 server 对吧？它的这个这个这个其实就不需要部署很多的这样一个触发消息的这样 thread，我部署一个就够了。其实现在最近有一些极端的这个使用案例，对吧？
17:51甚至说我要做一个 fully disaggregation 什么意思？他就说我的这个所有的这个这个这个 server 啊，它就不应该有 thread 啊，它就是说我就放一块内存在的。然后我的这个这个这个这个这个看去发这些操作，我们大家会看到，
18:05其实这些这个这个这个这个这样的话其实现在叫做一个 dissagted memory 的这样的一个设计。当然这个设计的话，我个人对为不大 cridal。因为你你有 RDM，你没有 CPU，这个是不大现实。
18:15但是这套方向它演化出来的另一个思路，就是说我能不能专门造一个硬件，它有这个一的能力，那这个就是类似于 dissavc CXL，对吧？大家可能也也有也有同学听说过的相关东西，
18:26它其实啊背后的这个演进路线大概就是这样。好，那我们先回过头来看一下这个是这个是是什么？是到底到底什么时候？我在在哪里里呢？那我在那边本质上它就是让网卡提供了一种硬件卸载的能力。
18:40这个操作啊，网卡帮你做完啊，那个叫什么 CPU 啊， CPU 就不用做了啊，不用做。但是这里面就会我们说设计这样一种能力的时候，
18:48如果大家就去设计这个 offloading 的这个系统对吧，一定会问到一个问题，就是你到底 offload 哪些操作？因为我们说应用的这个操作，它其实是有非常非常多的。你你如果 offdoor，
18:58然后然后然后但与此同时，你硬件上这个面积又比较有限啊，你放了一块操作器材，你其实就没有办法做另一块的这样的一个操作器材。所以这个 RTV 个设计者经过非常多的这样的一个思考，对吧？
19:10最后选择了三种言语。第一种就是 read 啊，很直接，我一个 server 可以 read 对面的这样的一个数据。这个对于比如说我查一些数据啊，我去做一些数据收集也好，
19:21其实非常有用的。那第二个是 right，就是我一个这个直接去写对面的这个 server 啊，这个你对于做一些数据库的这种拿啊啊这这个这个这个写 log 这种操是非常有用的。然后最后是 automatic 就是我我类似于 CCPU，对吧？
19:35大家想回想一下，我们说我们在讲这 OCC 的时候，我们讲过，上锁的上锁本质质是需要有一个抗带错的。那么网卡也可以提供这样的能力。但然的好处什么呢？
19:44就是说我的一些分布式锁啊，其实就是可以让网卡去实现了啊，然后最后它的甚至选了这三个原因。为什么这三个实现这三个原因以来，因因为这三个原因，我后还还看到它其实实现起来非常非常简单。
19:56它在现有的这个 q 三类的基础上，就稍微做一些扩展啊就能够去实现了。那这样的话，它就不会带来很多额外资源。好，那我们来看一个具体的这个这个这个这个这个这个实验例子。
20:10假设我们要实现一个 remote memory copy 啊， remote memcopy 就是什么意思呢？就是我我报了啊，需要把这个他自己的这样一块内存写到 i ice 的这样一块内存中。内存中啊，如果我们用之前的这个之前的这个 two 三 d 的这样的一个做法的话啊，
20:31它其实相对来说是非比较复杂，什么意思？就一开始我得像原原来一样，对吧？得完整的把这个 message 给发到这个私有这边，然后 message 包括我要写的这个内存的内容以及这个大小。
20:43然后呢，我这个 alice 这边啊得酷啊，一直在轮询啊。我说因为你不轮询的话，你延迟会很高。我轮询这个消息有没有收到？
20:50然后收到这个消息之后呢，我得把这个消息解包出来，然后去写这样的一个内存。然后最后发一个 ACK，给你的这个 server 啊，告诉呃给你的 bog 告诉你做完啊这东西非常复杂非常复杂。
21:02但是有了这个文三类的 RDA 之后呢，我们这个情情变得非常简单了。有了文三类的 RDA 之后，首先我们说 alice 这边什么事都不用干啊，注意 alice 是什么功能，它上面只需要你跑一个 RDA 的这种类似于管理者啊，
21:16注册一个 QV 就好了。然后 bob 呢如果要做一个 memory copy，他干了唯一一件事就是他往这个三 NQ 里面 pose 的一个请求。这个请求呢包含了这个远的个个要写的 RDMA 的这个地址啊，原来的内存地址以及他告诉网卡，这个请求是一个这个文赛的 librright，
21:35不是之前的一个 sap 好网卡之后，就就就会把所有的事情全部搞定，网卡直接做了。大家可能会想，网卡怎么怎么怎么那么神奇，对吧？
21:47但大家想想，网这个这个中其实没有什么神奇的网卡。如果能够发一个 q 三 d 的操作，它一定是能够发一个文 CID 的操作。什么意思啊？大家想想想我要干的事情是什么？
21:58无非不就是这个网卡需要把这个这个这个 right 的这个东西写到这个这个 alice 的这个内存里嘛。那么这个事情我们之前我但是我写一块内存，这个东西跟我写一个 message，这个消息本质上是没有什么区别的，对不对？所以在我三三的这个个它它这边，
22:16其实跟之前 c 三界的这个消息一样，它无非就是我先哎收到一请请求，我去看一下啊，这是个 right。然后我的 DMA 把你这个要写的这个数据读到网卡，读到网卡之后呢，
22:26网卡过一个硬化的这样的协议站。然后呢写到这个对面的这样色，本自后写完之后呢，哎他再用一个 DMA 写完就行。大家注意这个 DMA 都是什么？都是这个 PCIE 这个提供的这样的一个能力啊，
22:37是现成的啊，不需要去做额外的这样一个这样一个定制和事件。 OK 这样一套流程做完之后，我们其实就会发现一个事情，就是它的这个性能其实比 q 三 d 的要好很多。这边我们其实就对比了一下这个这个 RDMA 的 right 和那个叫什么 message。
22:55我们可以看到它的这个吞吐，实际上是这个整个 q 三 d 的两倍啊，这个这个这个这个前面我我们讲来来原原理，大家想也很简单。第一我的这个 server 对吧？它不是这个 neneck，
23:05原本你这 server 的这个 thread，它有很多的这个 pronect。第二个就是说我这个 right，它的本身它的发的东西更少。大家想想看，我们在这边的话，
23:12你这个 server 只需要写一个 right 这样样这个数据流就行了。如果你是初三列的话，你不仅要把这个消息读过来，你还得去铺这样的一个整个这个整个这个内存，对吧？它整体的这个这个消息就是底层的这个消息数量，
23:26实际上是会啊多很多的 OK。那这样导致的一个结果啊，就是我我如果用初塞列的这样一个呃文摘列这样的一个东西，就就就变变得很快快。好，那么到这个位置我 read 其实其实是一样的原理，
23:39对吧？我 PCI 既然可以 right，那我本质上也肯定可以免。那我 tommesic 稍微复杂一点啊，那我们后面再讲 OK，那么到这边为止我们就会发我其实这个问题有没有那么简单的啊，
23:52其实我们也还是有一些细节需要注意的，需要注意。比如说大家想想看，我们刚刚讲这个文塞内的对吧？它能够写对面的这个地址，我们是修的写法什么？
24:02就是我们说我们在这个阿 GB 请求里填了一个这个对面的这样的一个内存的地址。然后我这个网卡就非常神奇的把你的这是这个内存给了内存写了 OK，这是这样的一个这个这个啊这样的一个这样的一个事情。好，那么大家有没有想过一个事情，就是说这个你这个东西安不安全，
24:23对吧？这个东西你这么实现有没有问题啊？比如说我这个网卡，我再往你这里 address，填个零怎么办？我填个错的地址，
24:30我直接接把你对面的 server 内存全改坏了。那你这个东西不就没法 work 了吗？会不会出现这样的情况呢？那这种情况的话，这个 RDV 设计者当然也考虑到了，对吧？
24:40他为了跟其实跟 CPU 一样，对吧？为了做权限管控，其实 RDV 实现了一套叫做这个内存注册的地址。什么意思呢？就比如说 alice 啊，
24:51如果他想要让自己的一块内存啊被报读或者写，他得先告诉网卡这块内存是可读可写的那怎么做呢？ alice 就会调用一个叫做这个 reregister memory 的这样的一个接口。这样一个接口呢告诉告诉网卡 OK 这个你的这个这个这个这个这个这个这个某一块内存对应的这个地址啊，它是可读可写的。然后网卡呢它会自己的在自己的网卡上会存一个映射啊，
25:20就是映射到 s 哪些 visual memory，它是可以访问的以及他们的权限。然后这个映射做完之后呢，它会有一个 key 返还给了返还，返还给了。然后 bob 在做 RDMA 操作的时候，
25:32尤其是文三点的请求的时候，他得把这个 key 叫我们一般叫 RK 啊，叫 RP 去传给这个这个这个传到这个请求里面。然后当网卡收到这个请求的时候呢，他这边得做额外的时，就它不是一个简单的。
25:47就是说我直接我收到一个 RMPOK，我就直接把内存写了，并不是。然后他就说我先得表示吧，我这天先去查一下这个列表列表，然后我先检查一下你的这个权限，
25:55看有一个 key 来 match 啊，如果 match 了， matmatch 了，就啊它有有这样一套检测机制 OK。那这套机制其实不仅仅是对于写远端的内存有用，你对于写本地内存有效，
26:08对吧？因为比如说我们 IDF 触发一个 RDV 写我的这个网卡，得去 DMA 我这个这个本地的这个用户提供的这样一个 buffer，对吧？那这个 buffer 你也不能让网卡随便随便读，对吧？
26:19所以说我们这边也会有一个这个 local key 的这样的一个概念。所以我们区分一下会者是 RK 和 low key 啊， LKOK key 的话就去用来去访问那个发送者这样的内存。 RK 就是发送远端价格上好这边讲完之后啊，我们说这里面就是说你要实现文摘，也包括取代点区域。
26:36 ID 其实也是要 logo key，我们会发现啊这个事情相比较来说没有那么的简单，对吧？它会有这样的一些权限的这样一个控的这样的机制。有个这样一个 mamapping。但是这这里就会涉及到 RDM 的一个很经典的这样一个问题。
26:49就是说你这样一个 mapping 会带来什么代价？就这个 mapping，它保证了你的这个访问，它一定是经过检查的，是正确的。但是但是我们这样一个权限机制，
26:59它会带来什么样一个代江南。那么大家可以思考一个问题，就是我们现在每个 RDV 请求，我们先要去检测你的这个用户的这个数据是否合法。好，我们的做法是我们通过维护一个 making 的这样一个表，
27:13其实类似于操询页表一样，我们做这管理。但是这个页表要放在哪里呢？我们能不能直接放在网卡上？那像网卡的硬件有没有额外的存储能力呢？其实现在有一些网卡是有的啊，
27:25有些网卡有的。但是网卡的这个存储能力，它的这个这个这个这个这个 capability 啊，其实是比 CPU 差很多。什么意思啊？像现在的网卡一般，
27:34它只有一个这个类似于十兆的啊这样的一个这个这个最新的，应该也就十兆啊这样的一个内存去记这样的一个 mapping，以及其他的这样的一个连接的这样的信息。好，那这个时候大家想一想，如果我这个 mapping 很大，
27:49比如说我这个 s 注册了很多内存，对吧？就是它每一个页它其实它是其实跟类似于页表一样，就我每一个页它都会有一个这样一个映射以及这样的权限管理。那会这什么情况会出现你网卡存不下？好，
28:02那么网卡存不下就会出现什么情况？一种方式是说我我去这个不支持了，对吧？我的用户我不跑了啊，这个显然其实不就不可用。对用户来说，
28:12它其实并不并不容易接受，对吧？用户其实有时候希望我能跑，但是跑慢点没事，但是我我不得不能能，那那这时候网卡厂商怎么做呢？
28:20其实现在 RDV 的这种网卡的实现，采用的一个叫做 auto memory 的设计。什么意思呢？就是我网卡需要处理的这样的一个原信息。它一旦网卡的这个本地的这个内存 s rum，从这样它就会把这个内存数据放到这个 CPU 上，
28:36放到 CPU 上。然后呢，如果我正好需要那块数据的时候，我得什么我用这个 PCIE 去把它读回来。好，这个时候就出现了一个很大的问题，
28:48很大的问题。什么问题呢？就是如果你的这个这个这个这个这个这个这个这个这个一旦啊我在这边这个这个这个这个写 RDM right 的时候，如果我在网卡内，它没有办法去检查这个这个操作的这样的一个合理性，那就会出现一个这个这个这个这个额外的这个 PCI rn trip。
29:08而 PCI 的这个 round trip 呢，它其实从系统的这个建模的角度来分析啊，它其实对延始影响非常大的。我们可以来梳理一下整个这个 RDAV 这这个这个这这这这个 cycle 对吧？我们说你你 RDV 其求，无非就是我先发一个 MMIO，
29:22告诉网卡，我发操作啊，网卡做一个叠杯啊，去把这个数据读回来，然后再通过网络去发过去，然后占这个叠杯写好。
29:29我们说这个 MMIO 这个事情开销挺大的，两百到这个八百大表对吧？然后叠杯就是 PCIE，差不多要用微表，它占据了是吧？它占据了这样的一个整个楼曲的二点五表。
29:41大家注意啊，就是说我的这个点对点的这个凸信开销其实非常小。就是我在一个一个物理集群里面，比如说我在几十以内的这样一个集群里面，我的点缀的通信时间其实小于远小于一微秒。但是有个问题是你为了让不同设备这些凸信我走的这个 PCIE，
29:56它这个延迟其实反而会更大。它也接近于一微秒导致的一个结果。就是一旦我的这个 magic table，对吧？这个这个就这个这个这个这个超过了哎，我这个信任就变得很差了。
30:09 OK。好，那么我们简单总结一下。好，到这边为止呢，我们呃介绍完了，
30:16就是大概的就是 RDV 里面的这个整体的这样的一个协议啊，以及它的这个 q 三列的 VZ 列这种操作他们的好处，以及他们的一些实现的这样的这样的一个细节，一些实验实现的这样一个细节。那还不够啊，我们接下来会再带再带带大家看一下他们的 RDA 这个硬件底层的这些实际。
30:35因为你如果不不理解这些底层实验细节，你如果就就就随意用这这这些东西的话，它其实会会有很多的这样一个你预想不到的这个性能开销的啊，就是你的你就是所有现看看就是啊我预期这个网可能到一百 GPS，它比如说标的是一百 GP，一百 GBPS，
30:51但实际上我可能只能达到五十 GBPS，好不好？行好，那么我们再来回顾一下，我们前面讲了这样的一个我们在我们反复讲到左右交速其实非常非常重要。它就是整个这个 RD 杯的这样的一个整个 liack cycle。
31:13我们说我们三这个 RD 杯的这个核心的 cycle，就是说它为什么快？它的快的本质上来说，它在于 CPU 参与的部分非常少。什么意思呢？就是我们 CPU 参与的部分，
31:24其实只有里面第一步就是我调用 postsine 发起球这个过程。然后剩余的这样的一个这个这个这个整个过程，包括这个 DMA，包括这个这个这个这个这个这个这个这个远端 DMA，包括网络通信这些东西其实都是网卡做的，网卡做的好，
31:42那么这边就会有一个问题，就是那么站在一个应用的角度来说，对吧？我发一个这个网络，这个阿里用文赛列， right 它的时间是多少呢？
31:53其实我们可以看到你一个文赛列， right 这一整套流向。我们刚刚说它大概是在一到两微秒这样一个量级这样一个量级。这个东西它其实是非常难以隐藏。因为这些像一微秒，这个就是 PCI 的这个物理延迟去消不掉啊消不掉。
32:09然后你两个 PCI 延迟加上去就是两微秒， OK 就是这样的一个物理的延迟。好，那么这边就会有一个问题，就如果我有一个 CPU 框，假设我想要发文在这个阿里杯把这个带宽打满。
32:21我就是有一个 CPU 控，就是在这边我发完一文收到请求， complain 之后我再发下一个了。那大家想想这个时候它的吞吐能到多少呢？其实我们很容易算呢。就我们发现如果你有这样一种非常简单的这种模式去发发消息的话，
32:38你的这个一个核对吧，你其实也能只能达到一个门脸的这样一个请求的这样一个吞吐。然后这个其实其实非常不不的，现在的 RDBA 网卡其实能做几百美脸的，甚至八现在是最新已经到几千遍脸的这样的请求。每每个 cycle 那意味着什么？
32:52意味着说我们前面说你你的接收方的这个吞吐，我其实可以用这个文塞跟 RDV 去去把它打满。但是你的发送方它其实是没有办法把网卡的这个或者说你得你如果用这种那种非常简单的方式，那它是没有办法把网卡打满的那这个时候怎么办呢？其实一个核心的思想就是说啊我这个 CPU 发请求啊，它不能够等。
33:18什么意思呢？就是我发一个请求，我不能等这个网卡告诉我前一个请求做完啊，我才去发我我得什么我得连续的发，连续的发交给大家同学。比如说我我们前面说之前说 passas，
33:29对吧？ packsas，我们说我们要把这个这个这个这个请求发给一堆 acceptor。那如果你要把这个网卡用满的话，你不能什吧，你不能说我等一个 accept 回来，
33:40我我再去我再走。我应该什么？我应该连续的把这个所有的 accept 这个请求全发了 OK。然后发完之后呢，只要再再去等他们这样一个回来。那么这个其实本质上什么？
33:50就是一个系统里面经典的这个方法叫 overlapping，对吧？非常简单啊，非常简单，几乎所有的很作系统都要用 OK。那这一步能解决掉这个这个很多的这个等待的这个开销。
34:00但是大家注意 overlapping 用 overlapping 有一个很大的问题，就是我们就这个并行发的这个请求之间应该是要没有依赖的。就是如果如果我发前面这个消息，它依赖于前面一个这样的一个消息的结果，你是没有办法把它给并行起来了。 OK，
34:15那这个是第一个开。第一个方法就是说我们要用 RDV 的时候，它虽然网卡很快，但如果你 CPU 用的发的很慢，它也不够。为了让 CPU 发的快，
34:23我们得用这个其实也就是一个异步 l 嘛。就是我得让 IO 给 RDV，是还是需要去把这个 IO 给异步起来。 OK。好，那么这个是 RDV 里面。
34:33第一个需要说，也就是说我当我网网变快的时候，我的 CPU 测的这个发送的这个速率就很了。第二个问题就是能不能发现啊这个我我们我们说现在你发一一消息，对吧？它得是相当于是告得发一个消息通知网卡让网卡去读啊。
34:48我这个东西我们这个东西它其实在硬件实间上，像 MMIO 啊，在我们的驱动实限。大家如果去看的话，它其实就是写一个内存地址。好，
34:56但是有个问题，这个写内存它并不是传统的，我们写 CCPU 内存，为什么呢？因为这个内存它背后其实映射映射到了这个设备的这个 registely。因为说这个内写这个内存的开销，
35:09其实是要比写其他内存这个开销要大很多啊。我们这边统计了一下，写一个 MML 的这个内存开销，大概要在这个一千个 cycle 这样的接近也也也也接近半微秒级这样的这样的件事。那如果大家能做对比的话，就我一个随机原来 my marreaccess 的的这个开销最坏情况下，
35:29也就是大概是两三百个 cycle。所以我会发现你这个 MMIO 这个 cycle，它其实对于你的这个整体的这个吞吐影响其实还比较大的。那么我们有没有办法去减小这个 RDV 发那么多消息的这个 MMY cycle 呢？其实是有的，就是 RDV 啊，
35:46它有一些比较细腻的模式。比如说它有一个模式叫做叫做这个 dbble or batch。什么意思呢？就是说它网卡实际上是有能力一一把把你的这个假设我要发一把 batch 啊，就比如说我要像这种情况下，我要发一把请求，
36:01让网卡去做好。这个时候呢，网卡其实是有一个叫做 dbble batch。这实什么意思？就是我可以把这个 batatch 信息告诉我卡，然后这个信息我只需要一个 MII 发过学习了。
36:12然后说到这个消息之后呢，哎网卡它就直接用一个 DMA 啊，绕过过 CPU 这 n 个请求的这个消息啊，全都读回来，然后一起发。那这样的话，
36:21站在 CPU 的角度来说，我的这个 MMIO 的这样的一个开销又会被进一步减少。因为我 n 个请求只需要一个 docen batching 就可以了。那这个技术其实也挺也挺有意思啊，所以在在这个在在这个这边 paper 对吧？就是这边原始 paper 就简单刚 for RDA 啊，
36:39他其实就撤了。关于这种类似于 DMA 的这种方方式的话，它的这个这个这个这个呃的内存放的访问放大，以它的这个这个性能会比你的这个这个啊这个这个 MMO 要高很多。所以当我们把这两个优化放上去之后啊放上去之后，我们就会发现啊 base 就是一个最基本的，
36:58就是说就是我 RDV 发音请求收到它完全在做。然后右边实际上是我把这个 dobble bacsion 加起来啊去做的这样的一些我们会可以发现加上这些优化之后啊，我其实对于发送方来说，它的这个呃 RDV 的这个访问速率，那就就会高很多。这些技巧其实你不管是 CPU 能有哪一些，
37:17这些也是也是也是能用到 OK。那到这边位置的话，我们就会发现啊，有时候你 RDA 的接口看上去很简单，对吧？就是一个 postand 的这个 for comistion。
37:26但是你怎么去这个这个这个这个这个这个用，它其实其实是啊对你的性能其实非常相关的啊，这些相关其实它它其实跟你的这个硬件底层有关。所以说它其实是它相比较 TCPIP 啊，它其实并没有简化很多啊，这个方不是为什么啊？
37:41在前前前这个一五年到二零年之间，对吧？出现了很多这个 RDMA 化，对吧？因为 RDV 底层确实有很多这个优化的这样一个细节。我们来我们我需要注意，
37:53比如说我们再举一些例子，我们之前说了 RDMA，对吧？为了保为了检查你，为了为了使你的这个用户啊访问的这个内网卡，它能随意访问内存，
38:02然后呢我们又要保持正确性，怎么办呢？我们在网卡它得有足够的这个 permission check，就是检查检查信息，它会有一个页表 OK。但是我们除了这个页表以外，
38:13网卡只有一只有这个页表能不能操作呢？我们说其实是不行的，是检查。因为我们说网它它它本身是要把网络协议去卸载下来。那网络协议它本身也有一些原数据，比说我们前面讲的个 go back n 对吧？
38:24你到底是收到了第几个消息，这些东西你都是要存下来的。这些东西呢我们叫做这个这个这个 connection 啊，就是你每个 QP 实际上是有一些原原数据的这样信息的，包括我的这个检权限检查的这个翻译。然后我们前面讲过 RDV，
38:42它这些东西它其实都 RDV 处理的时候都都需要。但是有问题就是我这个网卡本身就是为了网络传输，它的这个硬件没有那么多这个资源去存这样的一些这个连接信息也好，这个 premission cheacch 信息也好，这时候怎么办呢？我们说 RDV 采用的是是是什么？
38:58是一个叫做 autumn 的解方译。就是说我的这个网卡它是只 catch 它当前的这个模控件，然后它大量的这个信息啊呃就是是放在内存里的。这是为什么？说大家去看现在这个很多阿里 v 网卡，对吧？
39:13它声称啊我能去实实现，比如说超过几呃几万的几百万的 QP 对吧？几百万的链接啊，但是原因很简单嘛，他把这些 QP 其实都放到了这个主机内存，对吧？
39:23主机内存显然这些东西都放不下，放不下。但是这样的话，这个这个这个种在 cash hit 的时候啊，这个东西其实很好。但我们前面讲过，
39:33你一旦 cash miss 了怎么办？我们就会这个这个这个这个这个有一个 PCIE 的这样的一个开箱。我们前面说了，就是你原本做一个请求，就是一个 PCIEB 的，它的呃就 dominant 的这个 overhead 来自于这个 PCIE。
39:48好，现在我当我开始 miss 之后，我还得再加一个这个 PCIE 的，加了一个 overhead。那这样其实大家想想这个导致一个结果什么导致一个结果就是这个这个这个这个我的技能基本上就会降半。就如果你的这个访问的这个力度，
40:03如果是这个你你网卡的这个，比如说你访问列表像是远超这个这个个这个这个用户的这个安且超网卡直播可以的啊。好，那这个时候怎么办呢？有一些方法是能够避免的，有一些方法是能够避免的。
40:17比如说 farm 这个二零一四年他这个工作人说什么？你你网卡不是要放这个列表翻译嘛，就是说我的这个用户的内存到网卡等访问的这个内存的这个地址的这个翻译，这个翻译项太多了，导致我重下怎么办呢？他当时用了一个 CPU 的这样一个技术，
40:34叫做大业啊，物理物理大业什么意思呢？就是就是就是就是就是我 CPU 原本是什么？四 k 为力度去去去做一个表象。那要这样的话，我的这个力度其实就是说你取决于你的这个这个这个这个总的大小除以四 k 对吧？
40:51那么在话不得说，以四 k 的这个力度太大了啊，太太小了。我为了减少表象怎么办呢？他说我的表象是一个 GB，一 GB 就就一个物理表象那样。
40:59在这样的话，我其实就能够减少数量级的这样的一个翻译 entry 啊，它其实就没这个问题。但是大家想想，我们用了一个一 GB 的这个 memory entry，有没有什有什么有什么有什么问题呢？
41:10它的一个问题是它的一个问题，就是说你的会有很多内存碎片。因为比如说我原本用户他可能上市，重新就用个几十兆这样的一个数据。但是为了减少这个列表翻译项，我得去分配一个 g 因为大家想这个翻译一定是得是一个连续的这样的一个内存。
41:26那我建议延续的话，那我一 GB 里面我只用了几十兆，那不是大量的空间就就浪费了嘛。 OK。所以呢它会有一些类似的这样的一个迭代。比如说比如说这个 light 啊，
41:38一七年有篇工作叫 light，他干的事情是说他发发现网卡有有时候访地址址需要翻译译，可可以直接访问物理地址。那怎么做呢？他发现如果你这个往 RDA 请求是由 linux 内核所发起的。因为内核它有所有的这个访问的这个权限，
41:56所以呢它就可以不用去去有这个翻译表，确保这个这个数据。那这个套方法它从根本上啊改变了这样的一个这个这个 RDA 翻译的这样我辑。但它会有一个问题，就是你所有的这个数据访问啊，它都得经过内核的这样的一个检查。
42:13所以它绕过了什么？绕过了我们说这个 RDV 这个坑和白 pass 的这样一个设计。所以它也并不是一个非常完美的这样的一个设计 OK。然后那么除了这个内存，其实有很多工作可以可以解决，对吧？
42:29比如说现在其实这个问题在这个 GPU 上也也有存在存在啊，但是 GPU 的可能提高比这个 CPU 稍微好一点，就 GPU 的这个力度没有 CPU 那么大。 GPU 的大概好像它默认的应该是六十四六十四六十四 k 一个一个表象吧，可能忘了，但它具体我忘了，
42:45但它比 CPU 大很多。但是 GPU 有一个问题，就是它好像是我记得啊是英伟达 GPU 是没有办法去像 CPU 那样去非常非常呃就控制叶子。比如说这个发部对吧？它可以说我一个 GB 啊，就是一个表象。
43:00但是 GPU 上好像我不知道按这个英伟达的那个驱动，有没有有没有提供这样一个能力。但是可能是没有这样的能力。但导致这个结果就是说这个问题其实在 GPU 上也可能是会会出现的，也可能会出现。但是现在 GPU 的话，
43:13它其实可以有一些方法 low。它比如说现在有很多设计，就是说我这个 GPU 它其实使用注册一块很小的这个内存。像我们我们之前搞那个那个大模型加载，对吧？也是这么多，
43:23就是我们在注册一块很小的内存。然后我们用这个 GPU，它本身的这个 on device 的有非常快的这个拷贝能力去做一个拍卖啊，能够去去规避掉这样的一个注册的这样的问题。但这个问题其实本身在在警 q 上也是也是有的。对对，
43:37 OK。那这个这个这个所以说大家在用八键 b 的时候，其实是是就需要注意一下这个问题。如果你让 GPU 注册一块非常大这样的一个内存的话，其实可能也是会有会有类似的这样一个问题的对。那除了 GPU 以外以外呢，
43:52其实我们说你你网卡它存的不仅仅是页软翻译，对吧？你还得存那个连接信息，对吧？你这个连接来了，你的这种丢包重传这个没有得到怎么做啊，
44:01这些东西其实是目前来看是非常非常难以销毁销销毁消毁的。比如说有一个片 picer 就做了一个实验啊，就是说它用那个相对来说比较新的啊，相对比较新的。现在已经到八了，就是说这个这个当我的这个网卡，
44:16它服务的这个网络连接的这个数量啊一旦变多啊，就会发现这个 RDV 这个 read 的性能是下。那它但单是呢就 read 的性能是下降， read right 不会下降。原因是因为这个 right 的 PCIE 和 read 的 PCI 是不打架的啊，这是一个非非常有意思的特性。
44:30那 anyway，那所以说这个这个这个这个这个这个有的工作呢就认为啊这个 RDV 啊这个连接信息这个东西影响很大。那怎么办呢？有一篇工作就是说就说哎那我能不能不用 RDV 的这个硬件的这种这种信息。比如说我就用这个 UD 啊来来去做通信。因为 UD 它有一一个常常的好好，
44:53大家回忆一下，我们说 URDV 有很多的这个连接种类，有就可靠的连接，有不可靠的连接，那不可靠靠的接接自顾名顾名思义对吧？它其实就不需要存这个连接的信息，
45:03它其实就没有没有我们前面所说的那一系列的这样一个问题，对吧？去参参加里面。然后他发现的有的工作就发现是说，我虽然用 UD 啊 UD 这个东西啊，它这个没有可靠性啊。
45:16但是呢我大部分有很多应用，比如说 RPC 它的这个上层的这个应用逻辑里面，它其实实现了一定程度的可靠性。因此呢，其实我我对这些应用啊，我不用 RC 去实现也也可以啊也可以。
45:28所以它这边就实现了一个类似于发 RPC 系统。其实我们可以看到，就是说当这个 RD 面它没有这个的呃这样的一个这个这个这个这个这个没有没有没有没有 scarity 的这个这个这个瓶颈的时候，它其实很快。但是一旦这个阿里 metrary 的瓶颈啊，导致就是说我的这个网卡需要从这个 sccreatp CI 去读这个数据的时候啊，
45:49它性能确实就会掉的挺厉害的。 OK 那这个也是一种一种那这个这个这个这个这这个这个设当当然基于 UD 的这种设计，就是说会有这个肯定会有很多问题嘛，对吧？第一个大家能想到问题，就是我我我 UD 会有可靠性的事情。
46:04但是 RDV 它其实有一个很有意思的点，就是说它虽然啊这个这个 UD 它不提供可靠性，但是用 UD 船的这个 RDV 包啊，它基本上是挺可靠的，为什么呢？为什么呢？
46:18就要讲到一些 RDV 的这个历史，就最早 RDV 啊，它这个这我们说它在 ID 对吧？是在超算啊里面去使用 OK，超算里面呢，它因为这个网络比较简单啊，
46:31所以它呢它可以实现一种叫做这个 no 呃叫做 loss less 的这个 pack transfer。就是说我的这个网络它是不丢包的，除非什么除非我这个包数据出错。那大家可能会好奇，对吧？就是我一个网络为什么会丢光了？
46:48假设我的东西都是好的，东西都是好的。其实网络现在的正常情况下丢包有两种情况。一种是说我的这个包坏了啊，就是说因为电波的干扰，我这个包坏了。
46:57还有情况呢，是这样的，就是你的网络，我们前面聊到这个交换机的架构，对吧？这网络是得通过交换机来去做佛理，
47:05但一旦你的这个交换机的这个把守不够了，就你你的交换机没有内存了，怎么办呢？其实现在的你就传统的以太网，它就简单就说哎我就丢包了，丢包。
47:15但是 RD 杯啊不行，阿 d 杯说因为我的这个端到端的这个网卡的这个丢包的这个这个这个处理能力相对来说比较弱。我们前面也看到，所以呢它有一个散粉，就是我告诉你这个你的 switch 不能丢包包，因为你交换机是本丢包。
47:28好，那么你交换机不能丢包的情况下的话，其实你在这个用油滴的这个情况下，它的丢包实际上是非常非常罕见的。因为你的这个因为电波干扰导致的这种包出错的这个事情，然后是非常非常低的。
47:40所以呢这边这个这个这这这个工作这这工作还挺有意思，他就测了一下，他说我大概传个五十个 PD 的油滴的包数据，大家才才会掉一个啊，它本质原因就是因为在更底下的这个网络层啊， switch，
47:54它其实会有呃这个原来相对的这个瑞 i 编程保证 OK。但是啊大家要注意啊，这个系统里面有一个很经典的，我觉得说就是任何你这个看上去给你的好的东西，它它都不是免费的。什么意思？
48:06 RTV，你看上去它这个这个整个 switch 给你的一个不丢包的这个这个感觉，对吧？比如说这个 loopy 的这个协议，他用了木呃，前前的的现用最新的可能呃忘了。
48:17那在以前用的一个一个方法叫做 pytht， fcontcontrol 是一套呃不丢包的协议。他干的事情很简单，就是他发现他这个 buffer 快存不下的时候，他就发一个信号，告诉你发的人说发慢点。
48:27你发卖点啊这个东西呢一旦规模上去，就会出现一个堆积的事情，就是我一个人发的慢了，然后呢，他那个慢慢还不够，他得是吧？
48:36他得发，好多人都让你好多人一起慢，就会导致你整个网络的这个发牢速度都会慢下来。就是说你所以说一旦出现一点点的 congestion 啊，在大规模下你的这个整个阿联诺网络就就性能就直接 class ass 掉，你跟你的这个应用一点关系都没有啊。
48:50这个实际上是是底层的这个网络协议，对吧？带来的这个额外的应用。所以最近几年其实也有也会有一些这个这个这个人在推这个这个就是老 lolossy 的这个这个 RDB 啊，就是说就是我的这个包啊，它它这个 switch 可以丢包。
49:05当然这个东西的话，它就需要端到端的一些更加复杂的这个配合啊，大家也很感兴趣，可以去看看最近的这个几年的这个 c com 和这个这个就是这个这个 NCC 的 paper，对吧？其实也挺非常多讨论这样一个 topic OK。
49:18其实当然这个 q 三 DRDB 还有好好处，就说它能减少这这个 RBB 的 long trip。就比如说这个语文三 DRDB，它只能做一个简单的读和写嘛。那如果你要做一些稍微复杂点的话，比比说你要做一个空的 chasing，
49:30就比如说你要遍立一棵别墅啊，那样的话你的王穴不会非常多啊，其实不大友好。但是你去三 d 的话，能够避免这样一种情况 OK 行，那这边位置的话，
49:40我们就讨论了另一个这样的一个 RDB 的这个实底层的实现带来的问题。就是它可能会有这个 scalability，尤其是你的这个连接数啊增多的这样的一个这个这个这个 scalability 这个问题。当然你不考虑这个 salabbility 问题的话，其实这个就我们在那 r 这边，其实还是啊还是从性能的角度上来，
50:01还是说还挺好好，我们在呃再休息会吧，你是嗯嗯你是嘘嘘嘘是嘘是你。