00:41是的，屁是是是是嗯去不是钱，我不是你放一首提示成为今天晚上有提况，没有什么按系统最最最那个什么，对不对？真是嗯，
05:15不，如果一不知道。好，我们继续 OK。那么我们这节课的话还是沿着上节课，就是说我们怎么去 scale 这个这个这个算力。
05:42对吧我们从硬件角度也好，软件角度也好，去系统的角度也好，去给这个这个设备提供这个更强这个个力力 k 啊上节课我们提到就是说你如果是一个单核的这样的一个 instruunction 的这个模型的话，它的这个算力其实是有一个明显的这个包 t neck 的。所以我们说你的这个设备现在一一种方式就是说我要用多核对吧，
06:02每个核去跑一个单线程的这样一个程序。那这种方式的话，它其实相对来说是一种相对容易的这个 sckilling 的这个方式。那这个方式的话，它在 sckilling 上面有两个 bot neck。第一个 btn neck 就是我有一个这个 cash coherence 的这样一个问题。
06:16因为我现在的这个 CPU 对吧？或者加速计算，我这个数据得放在这个 cathe。就导致一个问题就是我一个 catch 一个 CPU 的这样的一个 cathe 的这样的一个这个这个这个内容啊，它得及时广播到其他的所有的盒上好。那大家想想看，
06:33我们如果是有一个存储系统，对吧？在硬件里面，我们需要保证说我一个 CPU 写了一个东西，其他人都得看到。那么这个事情它是不是本质上来说贴上去，
06:45它的这个开销啊会随着你的这个核数越来越多，对吧？它是不是会显得变越来越越来越大？其实本质上来说也是也是这样的一个问题啊，就是说现在的这个 copelance 的协议其实都可以归成啊两类啊，一类协议叫做这个啊 soooopy base 啊，
07:03其实都 stoom base。所以意思呢就是说我每一个 CPU 盒啊，它会监监听这个内 u 总线。然后如果有另一个 CPU 盒啊，这个这个这个发一个做了一个修改。这个修改呢它会给 brown past ase 所有的这个 CPU 盒。
07:17然后哎我知道是另一个 CPU，就就说哦我这个独立的数据你不对了啊，这是一种叫 sooopy base。那这种它有一个问题，大家想想看，就是我的这个广播开销对吧？
07:26在硬件上的广播开销，实际上会随着你的这个核的数量越越多又是什么？因就说如果我要获得更大的 concurrency，我要获得更大的这个这个并行度。那我的这个 CPU 呃，我的这个 c 我的这个缓存的这个 coconcience 开销会根根根的增大，
07:42就导致一个结果。就当我的核多到一定程度，比如说现在 CPU 主要的只有二十几个核，二十几个核好这个这个这个我的 precas 的，它会达到我的这个计算的这个并行中带来的优势。会没有啊，
07:54这个会带来一个很严重的这样的一个问题。就是说一方面我计算上去，但是我的保存其实会更加的难。那还有一种方法叫做 direcct directbase ase。就是说我在这个 CPU 上有一个硬件，它记录了哪些核，
08:07它有最新的这样的一个数据。然后所有的 CPU 它要去访问这个数据，对吧？它都会去看一下这个 global direcch，就相当一个全局的这个协调者。那其实我们可以做一个核系，
08:17系统们可以看到当当有一个一个一个 base。不管你是系统呃硬件也好，软件也好，你在你的这个系统里面出现了一个全局的唯一的这样的一个人。我们就会发现他的 skculity 其实也会问题。因为你喜欢你一个全局者，
08:30他的这个性他就占了那么点硬件面积，对不对？他每秒能做的这种啊这个这个点上会查找设置的这个操作数量是非常有限的那这种其实他也不能够 skill 到非常好。所以我们就会发现一个问题，就是我要用多核啊多核去做的这样的一个这个这个这个就去去让它这个 crops 乘上去啊，它其实会有一个这样的一个很大的问题。
08:56就是说它得啊你没法设计一个非常高效的这样的一个快手非常型。然后这个时候大家其实我们也可以看到。另一点就是说如果大家对这个现在的这个硬件熟悉了，会发现哎，有一些处理器它其实核是挺多的。比如说华为的这个鲲鹏对吧？
09:11鲲鹏它其实是有六百四十个核啊，但更高的其实也有，包括英特尔，其实也有一个几百核的这种大机器。那大家有没有好奇说这种机器，它为什它为什么就是没有它不会受到这个 coherence 的影响呢？
09:26其原因很简单的，它就不提供 coherence rence 吧吧。如说像像它的的这个供的 cocoherence 就相对来说是比较弱的，比较弱。也就是说我这个一个 CPU 啊，它写了一个数据，
09:36另一个 CPU 它不一定能看见。那怎么看见呢？我得开发人员得什么得去手动的，在程序里面插入这种类似叫 fans 的这样的一个东西。那大家想想这个地方这个本质上是什么？本质上就是说我把这个 coherence 这个开销给转嫁到了这个开发人员，
09:54对不对？大家一起想想看，我们前面说你很多设备啊，我我要做算力的这个提升，它是有代价，代价什么？
10:01就是我开发人员得做更多的事情。那么这个是 CPU 的这样的一个情况，其实 GPU 也是类似的， GPU 本质上它内部也是有多核组成的那为什么 GPU 能够 skill 到更高的这个算的 GPU 的核数其实比 CPU 多很多，原因是在于 GPU，它其实完全没有提供 comference 啊，
10:21几乎没有提供 copearance。所以说它不会受到这样一个 preference 的这样的制援 OK。那这个我是我们说用多核的一个问题，就是你得除考虑 copearance。如果你不考虑 copearance，那开发人员得考虑开发人员得考虑那代价就是说我硬件标的这个算率提升了，
10:38但是你用户要用好它，得多花更多的这样的一个 coding 的成本也好，这个 knowledge 的成本也好，你得学 consistency model，对吧？我们我们这节课有讲过，
10:48还多什么都得学一遍，你才能用。那这个开代开发成本是挺高的。 OK。那除了这个这个除了这个这个这个这个这个呃多核以外呢，其啊除除了这个碑乐以外呢，
11:00多核其实还有一个大的问题，就是它的硬件资源其实是不是非常高效的。因为呢我们可以看啊，假设这是一块我们要做的这个芯片，对吧？好，
11:12现在我们说我们要做多核，多核是什么呢？就比如说我们假设我们要做四个多核，那也就说我每一个核上都有一个完整的，比如说 cofetch decode 啊， fetch fetchinstruction dedede de 方式这样一个方式 OK。
11:25我们每个核上都得有一块严格的这样一个东西。然后人们会发现一个事情，就是你的这个大 h 抵扣的这个东西啊，它其实是很占你的这个芯片上的面积的，很占很多片的面积。我们可以从这个图中可以看到，
11:40你的这个大 h 扣的，它其实占了一个非常单独里面这个非常大的面积。那么人们在想了，一个程中，那如果你是有多个核，那它的这个开销不就会随着你的这个核数去增加上去吧，
11:52对不对？那这时候人们就在想了一种方式是说我有我有一种方式啊，能够把这个我也是用多核。但是呢我希望能把这个这个这块单元给去掉，我把它的这个面积去留给更多的这样的一个计算计算单源啊，这个事情是可以做到的。
12:09这个就是下面一种非常经典的架构，叫做叫做这个 CMD 啊 CMD。它的一个核心观察时呢，就是说你在做那种需要做呃大量计算的操作应用。比如说你是一个这个向量，对吧？
12:22很多时候在是说我一条指我很多时候然后做的是同一个操同一个操作。只不过是在不同的这样的数据上。比如说我们前面讲的这样的一个向量加法的例子，它其实要做的指令就是一个加法。然后它唯一的不同就是说它加的这个数据是不同的。那么我们有没有一种办法，
12:39对吧？就是说我只提供一个这个加法，加加这个这个这个解析，我这个做的这个指令是什么的这样的一个器件啊。解析完之后呢，我绕这个不同的 ALU 啊，
12:51去操作不同的这样一个数据，不同这样数据那样的话不是也相当于一种并行嘛。那它比多核带来的好处就是说它在同样的面积下，它的这个 ALU 的数量会更多。大家想想我们一个设备上，对吧？
13:07它其实每个塞到底能产出，做到就在于不就取决于你这个芯片上面有多少个这种 ALU 嘛，对吧？本质上不就是就取于这样的一个 ALU。那这种方式的话，相比较前面那种多核的这种方式啊，
13:21它的这个计算效率，其实它的这个芯片面利用啊其实是非常好的听常了。那么这个东西的话，其实不仅仅是在这个这个这个 CPGGGU 本上，就是 CDB 本就就是啊我是最早最早期的 GPGPUOK。那么那么其实我们可以看到，
13:39就现在的不管是 CPU 也好， GPU 也好，它其实都会以这种 CND 的这种方式啊去提供这种并行的这样一个计算。比如说在英特尔啊里面，它其实就会啊提供这样的一个叫做 ABX 的这样的一个指令啊。这个指令呢它告诉你说 OK，
13:55我现在机器的一个核，它没能够做二百五十六个 bit 的这样的一个数的这样一个并行操作，意味着什么呢？就是比如说你我有四个 bit，一个一个四个 bit，一个一个数的话，
14:07那意味着着说我同时能够做二百五十六，除以四啊除以四这样的一个这个这个那么多个数可以同时做同做。然后呢，它的用起来其实相比较你的单线程啊，也稍微复杂一点点。什么意思啊？
14:20就是我先得去分配一块这个内存，告诉他 OK 这个东西是 CMD 能薄的。然后呢，我原本比如说我单核干的事情，比如我要加两个 banctor，我就把每一项都加上去。
14:31那么多核的话一样的，就是呢我去算一下，比如说我假设这个这个这个 CD 每秒啊，每个指令能做八个字点操作。那么我就算一下，就是 OK，
14:40我到底把这个路 p 数啊减少八倍。好，那么这个时候我就调一个这个 CD 的 m 八五六。也就也就是说它会告诉你啊，我这个二百五十六个 bit 的这个 banctor 是能够互相加的。 OK 就就干这样一件事。
14:53那这个事呢它的这个编程难度其实跟单线程其实差。虽然它会会比这个多线程稍微稍微简啊简单一点，但会会比单线程稍微复杂一点呢。但但是会比其实会比多线程简单一些，简单一些。然后同时它的这个能够在这个单核上去堆更多的相相同面积的情况下，
15:12那可以去堆更多的这样的一个计算单元 OK。那这种情况下的话也能够做到一个非常大的加速。比如说这个是我们在这个在我们这里实验室机器上我们测一下，对吧？就是说就加入多数啊，原本可能是单线程啊，
15:27我们要用这个 CFD 指令，五点七秒就完成了 OK，然后我加了这个 CMD，对吧？其实两点五秒就完成了，就完成了。
15:36好，那看上去是一个非常非常这个这个呃这个这个还是挺有效的这个方法对不对？ OK 那这个时候我们就会引出一大家大家大家没有想过一个很很很神奇的事情。就是说我们前面告诉他，大家说这个 CMD 啊，它一条指指令能够同做八八个八个计算。
15:53那理论上来说的话，我应该是有八倍的加速，对不对？但是为什么唉我最后测下来其实只有不到两倍的，其实就两倍左右，两倍多一点的这样一个加速，
16:04为什么呢？为什么呢？这其实就涉及到计算里面它另一个非常复杂的地方。就是说你的一个计算啊，它不仅仅取决于你的这个这个算的有多快，你其实是取决还取于什么。
16:17你的这个数据啊它到底能不能快速的跟上。因为大家想想我要计算的前提是什么？前提是我这个计算这个数据，它已经位到了我的这个计算的 ALU 里，对不对？但是一旦你的这个计我的这个这个算力设备没有喂饱 OK。
16:33那这个时候意味着什么？意味着说你的算力设备就会等在那边，不就会导致你的这个性能下降嘛。那具体来说的话，我们说你的一个计算，它其实是取决于它的这个数据访问去是取决于两个因素。
16:47第一个因素叫 lateny。就是说我去发起一个数据读取操作啊，到到我这个数据操作，它的返回的这个时间啊，这个 latency 本本上是很难克克服的。因为它因为你的任何速度嘛，
17:00它它不不能不不能超过光速。比如说我的一个 CPU，它要发起一个 drom 的这样的一个操作访问。那它至少会有一个几百个 CPU cycle 的，差不多就是一百纳秒的这样一个 ltacy OK。但是你光有 letacy 不够，
17:15因为比如说我们前面那个例子，你要算那个向量的加，对吧？你你要算一个很大的这个数据，很大的数据，为什么呢？
17:22你就是说你需要把那么多的数据全部都给读上来读上来。那读上来的话，大家要我读一个 better 的时读一 k 个 back 的的时间其实不一样，那取决于另个因素。就是这个 banad ways，就是当当我的一 mememarage 请求发到那台机，
17:37发到我的这个存储设备时候，对吧？我的这个这个这个这个我每个 bank 读上来啊，大概有多少？你这个时间是不能超过你的设备的这个带宽的那通常呢在这个在一个单机计算里面啊，带宽其实相对来说更啊，
17:52其实这两个因素啊其实都都都比较重要啊，它其实是看是看不同的这样的一个场景的。比如说你对一个 CPU 的这个 los store，对吧？它的呃它的因为它 CPUU 是呃一个个螺丝所发的嘛，所以其实你的这个这个延迟其实是是是非常重要。
18:06但是如果你去做一个非常大的这个数据传输的话，它其实延迟可能不一定很重要。它其实更重要的是什么？是这个带宽。那这里的话其实有一个很经典的例子，就是说大家想想假设我想要把一百个批比的这个数据，
18:23对吧？从这个上海传到北京，那大家想想我们要传这个数据的话，这个这个这个操作我们用哪种方法传会比较好呢？对吧？大家想一个非常简单的方，
18:33我能不能用手机？这个我上传就是我这个 wifi 去上传。那现在的 wifi 大家家想它的那这个我们说根据这样一个计算公式，对吧？我们说上海到北京，如果你是 wifi 的话啊，
18:44它的延迟大概是在两三百毫秒，这样一个量级其实很小，对不对？但是上海到北京这个带宽是多少呢？大家有没有想过这个事啊，这个带宽其实就是你的这个局域网的这个带宽，
18:54对吧？但我们就算它几几十兆每秒，大家想想，你看你这是一个一百 PB 的这样的一个数据，对吧？如果我用几十兆每秒，
19:03我可能要传个几百年，传个传三千瓦。那么我们有没有这种时候，我们有没有什么方法呢？其实一个现在业界提供的一个很一个方法，其是就是说我在这种带宽很大的场景的时，
19:17其实 latency 不重要，我 latency 可以长一点。但是我只需要什么，我只要提供足够大的带宽，那我什么什么情况下我能够提供一个最大的这样的一个带宽呢？就是这个是真实的云厂商服务，
19:28就 app 长他提供个叫做这个 so mobile 服。就是说你用户在一个云的数据 OK 它语音数据大家想想看，它无非就装在了几百块这个磁盘上，一百个 PB 对吧？几百块磁盘，然哦我要传数无非是吧？
19:40我就把你这整个磁盘都装载到一辆卡车上，然后这个卡车去去传。那大家看看这样的一个系系统里面，它的延迟是很高的。我从上海开到北京对吧？其实要开个八个小时，
19:52差不多它延迟八个小时。但是它的这个带宽其实趋近于无穷大，对吧？它带宽其实基本上是无穷大。因为什么其我八个小时就能把这个数据传了。所以说我们说你考虑这个考虑这个内访存的时候啊，
20:06要同时考虑这个这个这个计计算和计算和呃计算的速度和访存的反速度。那具体回到我们这边来算这个 back 的例子的话，我们其实就能很明显的看出当当你的这内内存的速度不快啊，不管你是延迟也好，不管你是缓存也好，但是在在 CPU 上很多时候尤其在超载，
20:23也没办法说是带宽比较重要一点啊，我们就会看到，比如说我要我们要做一个 vacd 加，对吧？不管你是 CMD 也好，不是 CMD 也好，
20:30它唯一干的事情什么？就是说我先把这个两个这个内存的这个给 load 到 CPU 的这个缓存里，然后我去加。好，大家想想想我们前面说过， CPU 里的这个 load 一次的这个代价是什么？
20:43是这个几百百个的那我做一次加法的这个操操作，只需需要一个塞口，一个大的就做完了。所以我会看到这个里面它其实是有一个非常大的这样的一个空呃，空炮在里面导致了一个结果。就是说我们之前那个叫什么这个这个这个这个这个这个虽然我们有我们每秒能多算八八倍的数，
21:02但是我们最后其实只只有获得了一倍的提升。当然大家其实大家可以看到的是，我们这个例子啊其实并没有那么简单，什么意思？就是现在的 CPU 啊，它不会那么傻的。
21:11就是说哎我这两块弄的好啊，我这个东西如果爱了，没法做，它就等在那边它会怎么做呢？它会做一个这个 prefetch，就是说我当我这个 r 二、
21:22 r 三啊，这个时候啊，就是说这个这个这个没读上来，我这个做不了时候，它会 CPU 会并行的去做 r 四 r 五。就是说这样一个类似于这样的一个 pipeline。
21:30那这样导致结果就是它的好处是什么呢？就当我这个循环做完，我再切出去，那我大概率二二二三这个数下一个二二二的数据据漏上来。但这个东西的话，它又取决于一个一个拔河效，
21:42是什么意思？就是说你到底是保存快还是保存的这个带宽快，还是计算带宽方法。因为大家想想我们给定一个 flops，其实本质上是吧，我们就能够算出一个计算的吞吐，
21:55对不对？然后我们给定你一个缓存的这个 pattern，我们就能够根据这个带宽去算一个缓存的吞吐。那大家想想我们这个系统啊，它因为因为你的不管你是计算吞吐也好，还是缓存吞吐也好，
22:08它都是硬件特性，你是不可能去超过它的。所以意味着说我们的这个系统啊，它的这个最优的性能一定是 bout net 在这两者中的一个最小的值。我们大家举个例子，比如说我们前面讲的这个 vector 的加法这个东西对吧？
22:22它本质上你的这个访存无非就是说我每秒假设我每个分每秒假设能做做那么多个，假设我们每秒啊我们的计算机是二点，假设我们的单核是二点二 g 赫兹，然后我们有八个 c md 的宽度是八，也意味着什么？意味着是说我们每秒其实是能做二百一十一 GB 的这样的一个数据。
22:42我们可以简单想想，就假设我们没有任何缓存的这个空洞。那大家再想想这个速度，其实这个其实是远大于什么远大于我们这样一个内存的这个 btt deack 的。所以意味着什么？意味着说在我们这样的一个应用里面，
22:57它的 border object 其实不仅仅在于这个这个这个这个这个这个计算，它其实还在于这样的一个这样一个缓存 OK。好，那么我们有了这样的一个概念，就是说我任何的这个计算对吧？它其实它其实可能会不仅受制于这个计算，
23:15它还可能会受制于保存。那我们到底我们怎么用一些系统化的方法去量纲这个应用，它到底是保存保存在还是去计算，保存在什什么样？这个时候我们就得介绍一个非常经典的这个模型，对吧？
23:29叫做 lofly model 啊， lofly model 啊，这个 model 呢基本上你只需要照着这个 model 一画 OK，我大概就能够知道，你就就我们就能一眼看出你这个计算，你这个应用到底是啊这个这个计算还是仿存保存呢？
23:41一旦我看出他到底是计算还是反思方呢，就是我只要什么去对应的增加这个能呃增加这个这个做一些设计就可以了。什么意呢？在 luu fly 包装里面呢，它的 y 轴代表的是这个这个每个应用啊，它观测到的这样的一个浮点计算，
23:59就它每秒做多少个浮点数。然后它其实是有个上界，就是说我这个硬件上对吧？它最高能达到的就是硬件一般标的那个浮点的这个上界。那么这样的一个上界的话，就是第一它有个横线标志，
24:12什么标志说我这个这个系统这个系统，它的这个这个这个算力它不能够达到这个。那如果我有一个应用，它的这个假设我们 x 轴是代表一个应用这个应用。好，我们发现它的这个达到的这个算力啊，
24:25就是我关注到的算力。在这个 look fly 的这个蓝线之下意味什么呢？意味说这个应用它其实还没有达到这个设备的算力啊，你还你应该在应用层做一些这样的优化，而不是这个设备的这样一个问题。好，
24:38那么这个是算力其实是比较好看的那问题来了，就是我们怎么去判断这个应用，对吧？它到底是这个它它到底带宽需求是多少呢？大家大家想这个其实本质上是在什么？就是我看一个应用的带宽需求，
24:53其实无非就是说我这个应用做一个计算需要读多少数据，对不对？那理论上来说，我一个应用做一个计算，我读的数据越少，那么它对于缓存的需求越低，
25:04对吧？那么有了这样的一个数值之呢，其实我们在里面啊它定义应用的这样的一个这个仿存需求啊，它是根据这个计算的效率来实实。所以在计算效率就是说哎我做多少次这个我我做一次计算需要读多少个数据。比如说我是可以读八分之一个数据，
25:22读四分之一个数据，读二分之一个数据，读读一读一个一个 back 的数据的，以此类推。这个时候我们其实很明显的看到，就是当你的这个应用啊，
25:30你的这个啊这个这个这个这个这个我每个计算要读的这个数据，它其实越少，对吧？越少它的这个这个这个这个这个每个 flows back，就是我每个或者说反过来，就是说我每一个 bit 对吧？
25:43我每读一个 bit，它们做的这个计算越多，那其实代表什么？代表着我的这个计算利用率越高 OK。那有了这两个定义之后呢，我们就会发现一个很巧妙的事情，
25:54就我们把它除一除啊，它就会发现这个斜率啊代表的什么，就是我给定一个应用这个计算 OK。然后呢，我其实就能够画出什么画出这个这个这个我要把这个算力吃满所所需要的这个访存带宽是多少？然后呢，
26:11我们每一个设备，它其实会有一个自己的这样的一个这个这个这个这个斜线代表了什么？代表的就是我这一个设备啊，它给这个计算，它提供的这个带宽带宽是多少？好，
26:24我们可以这个时候大家稍微想想啊，我们这个时候我们就其实画出了两条线啊，一条线斜线代表的是什么呢？代表的是这个设备的这个访存带宽啊，以及我这个应对对这个设备的防存的需求。然后另一条线横线代表的是这个这个设备的这样一个算力的这个需求。
26:42然后我们给另一个应用，其实我们是可以通过这个分析它的代码，我们会知道对吧？它这个每读一个 bit 对吧？它要做它能做多少次浮点的这样的一个操作。那这种情况下，
26:54我们给定一个应用的这个 art aratatic fic intensity。我们其实就可以看到，如果它碰到了这个纺存带宽意味着什么？意味着说这个看到这个斜线意味什么，意味说这个应用它一定是 memory btt deck。因为它如果你要超过，
27:10如果你的这个所获得的这个 flops 啊，超过了这样的一个线就代表什么？你就要代表你个设备的这个这个保存它它变高了，这是不可能的。因为你的设备它的这个呃呃存多少，实际上是硬硬件限制一样的。
27:22如果果我一个设备在右边，对吧？我读读半年，我能做很多计算。那么它最终会保证在它在什么？在这个 pick for frops 上，
27:29就是说你的这个设备的算力不够了啊，你算不过来了啊，那有了这样的一个 model 的话，我们其实就是能够很简单的去去看出，对吧？我运用它到底是受制于保存还是受制于这样的一个算力。
27:42那对于我们刚刚那个 vack add 这样一个程序，对吧？它本质上就是受于这这的一个保存，那这个时候怎么办呢？那就是吧那其实就有很多种方法，第一种方法就是什么？
27:51我可以我是不是可以把这个设备的这个保存再变快一点，把这个斜率变高，对吧？一旦呃变高，但是一旦你这个斜率变高，我的这个同样的这个横线，
28:00它能获得的算力就是没算力不就变得更高了嘛。那这种方一般来说是我换一些更强硬件对吧？比如说 GPU 对吧？ GPU 它的算力很强。那么我我我我我大家家看 GPU 上的内存跟 CPU 上不一样，对吧？
28:13 CCPU 都是这个 dirum 对吧？那 GG CPU 大家可能也听说过叫做 HDM，对吧？ high valand ous family 啊，它其实就是带宽会比 CPU 的要高高很多。就是因为它要增加这个缓存缓存，
28:24当然你还可以做一些软件层略优化。比如说我是不是能让这个应用的这个 automauthatic intensity 往右移，就说我能够在相同的 bit 内去做更多的这个计算，对吧？那这个就需要你做一些这个这个算法上的改进，那这个就比较复杂，
28:38非非常干脏应用，我们可能就不考虑。那么呢还那如果你到算力频率怎么办呢？一种就是说我去加更多的这个算计设备，对吧？比如说我去啊，
28:48我去搞搞啊，我去去去买新一代 GPU 吧 OK，那这就是另外的事。 OK 好好，那么到这边为止的话，我们其实就把 CMD 啊 CMDT 给大家介绍一下。
29:00那么在这道 CMD 它本身是一个很简单的概念啊，但是在讲 CMD 的时候，我们其实给大家引入一个很重要是就是说我们要考虑你这个设备的计算能力，对吧？我们不仅仅啊要考虑你这个设备，它到底这个算的多快，
29:12我们还要考虑这个存的多快。那么到这边为止，其实我们就把最基本的人们构造硬件的这样 skilling 的这个方法其实讲差不多了。包括最早期的这样一个 GPGPU，其实也是这样的一个概念。大家啊你包括英伟达早期的这个 GPU，
29:30对吧？它的其实也是一个大的 CND 嘛，是你加上一个多核去组合在一起的。它跟 CPU 唯一的不同，就是说它的 CND 的宽度是是要比 CPU 大很多的。比如说 CPU 它它一般英特尔的话，
29:42现在是八个 c 八每秒做八个这个 ALV 嘛。但是但是英伟达的话， GPU 的话，它的库达克这个叫库达克，对吧？其实会高很多，
29:49比如三十二啊三十二以及更多的这样的一个库达克做这样一个实例。那么大家这时候其实就好奇的就是说，为啥这个这个英伟达这么牛逼逼，能够啊堆那么多的这个 CCD 的核，为什么这个这个 CPU 搞不定，对吧？
30:03那其实这个其实也并不是说这个英伟达的科技学位比比英英卡高一个炫，对吧？基本上是英英伟达做了一些吹豆腐，比如说英伟达的这个 ACMD 上是没有开始 preference 的，所以他可以把那一部分的硬件给腾出来去做做做做做做 CMD。然后英伟达的这个这个这个这个这个卡上是没有，
30:24我不知道最新代有没有，但是之前肯定是没有这个 branch predict。就是说我做分区预测啊，这个其实在 CPU 上很重要，但是在 GPU 上是没有那么留下来的。这个更多的单元，
30:35我就可以做这样的一个这个这个这个这个这个算力 OK。那这这两种把它去做了一个这个这个这个这个搞出来之后，对吧？其实你可以把它做一个任意的这样的一个组合。比如说你的这个这个这个这个 i 九对吧？是英特尔的比较强的这样的一个 CPU。
30:52那它其实就是一个八个核，然后每个核有八个 CMD，那比如说英伟达的这样的一个 GTS，对吧？它其实就是十五个核，然后每个核有这个三十二个这样的 CCD。
31:02那看上去它的这样一个计算密度其实就会高很多。其计算率的本账什么就是在于你的这个这个上目到底能对多少 LU，对不对？是，当然你的这个这个姐这个这个 ALU 它这个堆的啊，不是不是没有代价的对吧？
31:20比如说在英特尔的这个或者说在传统的这个 CPU 上，对吧？你有大量的这个芯片的单元给这个控制给这个 cash，对吧？但是在这个 GPU 上，我为了算力算力，
31:32这个这个这个我基本上是基本上就没有这些东西了。 OK。好，这个时候大家不知道有没有好奇过的，我曾经也很好奇的，就是说那有了 CMD 之后，
31:41对吧？我需不需要这个这个为什么我还需要毛推扩呢？就比如说我一个 CMD，为什么我只做三十二个，对吧？我或者六十四个或者一百个，
31:49我不我不去做客多了，因为简单来说，我有两个毛推扩，两个毛推扩，然后就是说两两两两两两两两个 modecall。那理论上说我也可以做一个新 d 它的这个 AOU 数量是之前毛 t 扣的两倍，
32:02它其实其实还能做更多。因为我我我可以少另一个这样一个 moli code 这样的一个计算。那个那个控制流对不对啊？这个这个个其实其其是其实后面我得到解答，它其实是有两两个原因啊，两个原因。
32:16第一个原因呢就是说 CND 啊，它这个东西它要保证什么？它要保证我所有的 ALU 在同一时刻都在做同一个操作。同一个操作。这个事情呢会给这个硬件设计上带来一个限制，什么限制，
32:30大家还是可以来看这样一个效果。就比如说啊就是我问了之前做做做芯片的这个这个这个这个这个老师，就是大家想想如果我要把一整块芯片全部把它做成这个这个 CD，就会出现说我最左边的一个 ALU 和最右边那个 ALU 啊，它们之间会隔一个很大的这个距离，对吧？
32:54然后我们说你在这个芯片上，它芯片你那么大一块芯片，你要把信号传过去，其实是要花一定花一定时间的。也就是说你当你的这个距离越大，它这个误差就就会越大，
33:06就导致致这个结果。就当你把一整块芯片全部做成 CBD 的时候，好就会发现一个事情。就是我这两块芯片这个这个这个他们在完成同一个操作时间上会有略微的这个不同。那这个不同会让这个芯片的这个整体的这个设计会复杂，非常非常多。
33:24所以其实一种做法就是说哎我就不要改那么大嘛，就是我把这个分成四块，对吧？每一块只要跟在块内去做这个 CND 的这个同步就可以了。那这是其中一个原因。那还有一个原因，
33:37我们大然有看到你 CND 有个问题，就是我所有的 LU 都得去同一直间做同一个这样的一个操作。但是如果我有一些应用，它有分支怎么办？你有分支的话，那我 CMD 其实是是支持起来是非常糟糕的。
33:51这个时候如果我用这种多核的这样的一个这样的一个设计的话，它其实在控制流上啊相对来说会会更加的 flexible 一点啊， flexible 点。好，那我们就来看看这个问题，就是你的这个 c md 对吧？
34:07我们现在说 CMD 的假设是什么？就是说我所有要用同一时间都要执行同位效指令。那我如果假设我们的这个它同位效指令它的好处吧，就是说我所有的 BL 都卸了的一个 problem model，它就没有这个 fetch 以的这样一个额外的这样的硬件逻辑上的这样一个资源这个开销。但是问题是如果我们要有一个 conditional 的这样的一个操作，
34:30我们就是说我们我们不同，比如说我们 CVD 原本是为了操作一个 vector，对不对？但是我们要操这个 vector，因为我们说不同的这个 vector 的 entry 啊，就是有它是要根据这个当前的 vector 的值做一个不同的这个操作。
34:44怎么办？比如说我说大大于我才才才做一些操作，这个其实大家想很常见。我说大冒性激活不也是嘛，就是说我会把那些这个这个最高的那些这个这个高的就是大于零。就比如说我我是 rereview，
34:55对吧？ review 的话，它干事情就就是说我大于零的这个向量值，我把它留下来啊，小于零的，我小于等于零的，
35:02我就把它全部变成零 OK。那大家还有这个操作，用一个 CMD，我每个一条指令只能做同一个事情来说，它其实是没有办法做的那这个时候怎么办呢？怎么办呢？
35:13其实就是本质上就是我有分支，我怎么做呢？这个时候有没有想到就是说 CMD 啊，它其实有有一种执行分支方法是什么什么意思啊？就是我分支无非就是我有好几个 branch 吧。那我其实把每个 branch 都执行一遍。
35:27然后呢，我在执行某一个 branch 的时候，我只把这些满足 branch 条件的那些这个这个这个这个修改啊，把它给写回，其他修改啊，我就不写回。
35:36那这个本质上来说，不就最终等价于我做了一个 branch 的这样的操作，多少时才能操作吗？这这是一个非常强大的设计啊，我觉得大家可以看一下，比如说我们有个例子，
35:47假设我们有一个 vetor，就是有八个这个 vector，假设我们需要用 CMD 一次操作八个 entry。好，然后呢，我们是这个 entry 呢，
35:56我们就类似于刚刚讲的这个这个这个 review，对吧？就是说类似的像法就说 x 大于零，我做的是一个 x potential 的一个计算，然后呢， x 小于零小于等于零，
36:05我做的是另一个这样一个计算。那么我们说对于 CMD 来说，我同一时间对吧？我所有的这个 entry 我都只能做同一个操作，那我怎么去实现这样一个 brunch 呢？那么其实它的做法就是我加一个 mask 什么意思？
36:18就是说我在执行第一个 branch 的时候，我会生成一下，说生成一堆 mask。这 bus 告诉你说，在第一个第二个和第三个 entry 里面啊，它的第一条第一个这个 branch 的执行结果是要写回的。
36:31好，然后做完做完之后呢，我就做第二个 branch。在第二个 branch 时候呢，我就把第一个 branch true 的这些东西啊就给制成 false，然后把 for 之前 false 变成 true，
36:41意味着说什么呢？我第二个 brunch 只对于那些生成 true 的这些操作啊，去做一个协会。好，这样大家想想我当我们们类把这个 eentic ND 的这两个 brrouch 都给执行完了，就就会发生什么事，
36:54就就会发生。其实其实就是说我小于零的对吧？大于大于零的对吧？也做小于零的，什么也做也做了。但是大家想想这种方式，
37:03这种方式其实是能够支持任意的这样一个 branch，对吧？但是它会引入两个这样的一个又会引入两个新的这个问题。那第一个问题其实我们很明显能看到它其实会很大程度的降低 CMD 的效率。大家想想原本我如果正常情况下的话，我应该是什么？
37:19应该是每个核去执行它当前的的这个 branch，就是说我正常应该是三条指令就结束了。但是如果你是一个这个这个 CMD 的执行，你要你要做这成 branch，我得把所有的 branch 都执行一遍。那如果呃你的这个 branch 非常非常多，
37:32那我的这个执行开销不就乘以翻倍了。而且大家可以看到我们这边打叉的这些东西的话，其实都是说被 mask 掉的这样的一个计算，也就是什么？这些计算其实是浪费的。所以呢在英伟达的手册上，
37:44它其实讲过就很很很很听听，懂吗？就是说当你发现啊它在这个扩大上那个叫什么效率，不是扩大是英伟达的这个框架。那会说这个这个这个编程的这个这个很多时候你的算力没有用满，很大程度上就是因为你这个这个考勤系都不让去你的这个分支啊搞太多了啊，
38:01所以他的这个计算量效率变差。当然这个 simd 还有一个问题，就是就是开发问题，大家可以可以看看啊，就是我们什么时候这个 CMD，对吧？
38:10就是说呃我们为了实现 condition branch，我们正常来说开发人员写的是这个这样的一套这个操作，对吧？那实际上你为了实现这个效果，你得去手动的加这个 mask 啊这些操作。那这些操作不都是得写额外的这样的一个程序程。
38:25这个开发额外的开发成本嘛。我们说这它是一个 trade off OK。那这个时候我就不得不提到一个东西，叫 CND 啊，大家可能会听说过，对吧？
38:34叫 single instruction multiple thread。这个东西呢它其实就是英伟达提了一个英伟达成功。非常重要的一点就是它的它提了一个编程的这个抽象。什么意思呢？在英伟达的这个编程上的 CMT 里啊，你开发人员还是只需要写这种东西就可以了。
38:50然后呢，这个编译器啊，它会自动的去帮你把这个 mask 给加上去。虽然这个技术点不是特别特别的这个你的新因为你你其实可以看到一九八几的时候，英英特尔的这个这个这个这个 CPU 的这个 CMD 上，其实也是也也是有类似的东西的啊。
39:08当当时应该叫 silk 编译器吧，大家感觉下当然是个老古董啊，但是其实也有类似东西。那核心什么呢？就是说我的开发员啊，只需要把它把你的这个 CMD 的这个机器啊，
39:20当成一个多核的机器器，多核的机器器。然后呢，然后我这个佑达的这个这个编译器会自动会帮你做这些的卡机系统的这个操作。那具体来说的话，在这个英伟达的这个 CUD 的这样的一个这个这个这个这个这个机器里面，
39:37它的这个这个每一个 ALU 啊，它都给你抽象成了一个现实。每个 AO 都创造成一个线程。看这 thread，这个其实对大家来说是非常是因为我们这个编程的这个演化史，对吧？
39:50是从 single thread 到 multithread 的这样一个演进的那这个大家其实非常熟悉的。然后呢，我们每个 thread 它会有个独立的这个标识符啊标识符。然后告诉你就是说哎我这个到底是在这个机器上的第几个计算单元。那这样的话我应用就可以干个个什么啊？就是我可以根据你的这个 threi d 啊，
40:06其实会去能能去拿对应的这个数据 OK。然后你每个 thread 里面跑的就是一个正常的 c 的，这样有 general purpose 的代码。所以大家看现在 GPU 其实叫 GPGPU，对吧？就是说啊 general persons 的这样的一个 GPUOK。
40:22好，那这样的话，然后你如果要你你在这个 c 里面是可以写 prunch 的。然后英伟达的这个编译器会自动的帮你去加这种 mask 的操作，把这事划定。那这样的话其实就非常方便。
40:32当然这个只是一个最基本的这样 CMT 假设。实际上英伟达的这个扩大其实变得非常复杂。因为它它其实会把 thread 就是给给设计成一个海 o 比这种不同的 thread，它会分成这个 thread brock 对吧？然后 thread lock 之间又会分成这个，这个应该是分成这个啊分成分成哎，
40:52呃反正就就不同 spread 就会组成科冷嘛。现在其实非常非常非常复杂，但它的逻辑上其实并你其实很直观的。因为大家想想我每每个 spread 其实对应的是什么？对应的是这个硬件上的一个 ALU，然后不同 ALU 之间对吧？
41:05它的其实是位置是不一样的。我们在同一块硬件的这样的一个，比如说毛推裤靠上的这个 ALU 啊，它的这个同步其实会更快一点。所以它是基于这样的一个硬件的这个海 erary 去提供的这样的一个这样来在具体具体的细节啊，我们有时间再讲今天那今今天今天这个我们应该就不讲那么细节了。
41:24 OK 那那回过头来的话，那有了这样的一个每个这个这个 CNT 的这样的操作之后啊，我们去做这样一个并行操作。就跟写毛坯货其实是一样。比如说我要去加这个这个这个这个这个这个这个这个这个这个这个我就是还是我们之前的做一个向量加的这样的一个操作，对吧？
41:42我们要做 CMT，其实就非常简单。就是说我是这个这个这个每个 thread 就去加一个数就行了，每个 thread 去就去就去加一个数。然后呢我我去捞取一个 GPU 的坑呢，它干的事就是说啊我会尽可能的用这个 GPU 上所有的这样的一个 thread 啊去去加 OK 加。
42:01那这样的话我的这个这个计算效率其实相当于来说就是就非常非常高了。对，大家可以看到是我们当时也测过，对吧？就是我们拿了一个非常挫的 GPU 啊，也也不能算特别挫吧，
42:13但是非常非常旧了，就 v 一百啊，我们就看一下你去 v 一百，你去搞这样一个一万个数这个加法，对吧？其实在二十八微秒内就可以完成啊，
42:21但是你在 CPU 上我就使有 ABX，我继续用多核对吧。其实只能做到两点啊啊都很大的影响啊。但凡就是能做到两点五毫秒，其实看到还是有一个非常的这样的一个大的差距。当然这个这个这个事情本身它背后因为其实做了很多很多的这样或者是硬件调度上也好，
42:39这个软件的运取上的也好，这样这些这些这些优化啊其实是是看不见。但是所以但是本本本质上来说的话，有了这样的一个更多的这个 CD，对吧？有这啊其实我们就能把这个这个整整体值的计算效率啊提升很多 mumulticco k 事情没事。
43:01好，那么我们总结一下啊，总结一下。那那到这边为止呢，我们就讲了在这个通用编程时代，注意是通用编程时代。
43:10这个这个我如果要有一块芯片啊，我怎么让它的这个算力变得更高的这样的一个方式。那对于你一个单核来说，他能干的事情无非就是拍卖加超标量，然后加加提升主频提升到这个不能提升为止。那么当你的单核这个没有办法进一步提升之后呢，
43:29我就要用 CMD，就是说我让这个单核上面我们就只加计算单元，但不加其他的这个东西啊。最后的话，当我一个 CND，它其实是有这个 control plus 的这样的一个就是它这个控制流对吧？
43:42其实不高效，以及我们说 CND 不能够建很多，我不可能说我一个芯片上全都是给你做这个 CND 的这个 LU。所以呢这个时候我们就会有这样的一 mumulticcore 这样的一个架构。就是我们把很多这个多核就把一个一个大芯片呢去划分成不同的格子啊，不同的格每个格子里面去搞一个小气垫。
44:01那这样的话就其实主要就是这三种演化方式。那么我们还有没有更进一步的这样的一个演化方式呢？实际上是有的啊，有的啊，有的就是说其实我们一直讲的这个叫 DSA。就是说当你的这个这个这个我们这三步都满足不了的时候，
44:20我们就需要为什么为一个特定的计算去设定一个单元。这个在在在 a 时代其是非常非常流行啊，其实基一个非常重要的 DSA 就是什么？就是矩阵乘法。我们在大家在回过头来想一对我们讲的这的这一开头我们讲的这个例子说你的大模型不管是训练还好，推理也好，
44:39这里面的其实计算的的这个 motornag 很大的这个操作。什么是这个矩阵乘法？那么我们能不能搞一些硬件，对吧？这个硬件只能做矩阵乘法。因为大家想，
44:48即使你有 CMD，你要做一个矩阵乘法，你还是得写一个这个 for 的循环，对不对？然后在那边狂哐哐框搞。你需要让你需要很多个 CMD 的这种组合，
44:58你才能去搞出一个矩阵乘法。但是如果我们 happily 我们这个机器上有一些很强的矩个硬件。比如说我就告诉你我就发一条指令，我就能在一个 cycle 啊。但是一般现在做不到一个 cycle，但应该是几个几个 cycle。
45:10会员非常小，就能够完清的聚子阵法。那这个在体实结构里面就是有一些非常经典的设计，叫这叫脉冲阵列，对吧？它其实是其实是它能够做到就是我相同的硬件的面积，
45:21我只能用更少的这个 cycle，就是这个 cycle 数去出这样一个聚止阵法。那这个东西其实我们知道道 omeic basic 策略就是说我只为你这一个服务做一个硬件啊，其实现在的这个不管是 CPU 也好， GPU 也好，其实配配备硬件，
45:38比如说 CPU 英特尔的英特尔的话提供了一个叫做的这个加速器。然后最近其实还有 NPU，英特尔 NPU 它本质上啊或者是端侧的 NPU，它其实本质上就是一个这个这个矩阵尺法的这样的加速器啊。 GPU 啊，大家可能都听说过一个东西叫探 cl call，
45:55对吧？ tcl o 其实本质上就是 GPU 里面的这样的一个矩阵加速器，其实最近几年是越越做越大越做越大。然后然后我们前面也提到过 TPU，就 google 这个 TPU，那它本它这个 TPU 跟 GPUCPU 什么呢？
46:08它的本质是就在这个 TPU 实里上比大 PU 的这个面积都给了这个这个做矩阵乘法的这个加速器。所以说它们在算矩阵乘法的时候，它的这个啊 flops 实际上是肯定是比 CPU 和 GPU 更高的、更高的、更高的。当然这个因为这个东西非常非常火，
46:26非常非常火。所以说这个这这个不同的厂商对吧？其实都都有都有在推。对对，好，但是这个时候呢大家会想有没有想过一个很很经典的问题，
46:35就是那既然我有一个操作，对吧？比如说程法，我如果把它硬化到了硬件里面，都能够获得更好的这样的一个性能提升的话，那我为什么不把所有的这个计算对吧？
46:47都把它硬化下来，我们要写软件干嘛？我为什么要写 CI 加，后面要写拍仓，我们直接做一个硬件不就行了吗？这个其实在于一个非常重要的点，
46:56就是是系统里面非常重要的一个概念。就是你做任何的这个 specialization，它不是 free。尤其是在近些年就是说大家可以想象，一就是现在我要造一个硬件，造一个硬件。
47:11这个事情本身它不是一个简简单单的说我在写个 IPGA，然后跑到模拟器结束了。其实并不是你要让这个硬件真正的 work 肯定要去留片。留片的话，如果你要留个什么啊，六十五纳米这种蹉坨的制程其实是没有问题的。
47:25但大家想想，如果你用六十五纳米去留一个 DSA，对吧？它的这个速度其实是比不上你在一个 CPU 或者整个 GPGPU 上，或者这个五纳米的这个这个设备上去去去去直接跑的。我虽然我虽然这个整个我是要花更多的指令，
47:41但我毕竟这指令快，对不对？那我能不能在这五纳米或者七纳米这个上面去做一个 DSC 呢？这个事情被证明是非常非常困难的。原因就很简单了，就你要花钱，
47:51就是说现在的这个工艺啊，它已经复杂到一定程度，就是你油片，你如果这个这个这个你你要你要给一次片，你几十一刀啊，这种几几啊啊具体数量我可能不是特别清楚。
48:02但是啊这里面可以看到的就是你好多好多美刀，对吧？就得花出去。所以这就会带来一个非常大的这个就是限制。就是说就是说很多时候啊，我要把一个 DAC 真正的让他 work，
48:18我得花很多的这样一个成本，我就花很多的成本。然后呢，这当我当我这个这个这个成本太高的时候，我如果不能赚回回益益，基本上能够赔钱钱的这样一个卖卖后呢，
48:29如果我有一些比较便宜的 d 制程去做 DAC，对吧？它的这个效率其实不一定能够比得上我一个这个就是说说用计算算器一个非常高计程的这个通用计算器的这样一个上面做事。所以我们说你做 specialization，那 specialization 就硬件的 specialization 是一种非常方便，能提供提高这个软件性能的这样或者是系统性能的这样一个方式。
48:53但这种方式的话一定得去衡量，就是说你这个事情到底能有多大的带来多大的收益。那你看大家现在可以看到是现在主流的这个 DAC 其实有哪几个。其实我们这节课刚刚讲过一个就是什么矩阵神法，这个是大家毋庸置疑的，都是非常重要的这个东西。
49:08所以不同厂商都把它给硬化。但是除了举举个情况，其实我们这我们这这 CSP 还讲过一个很重要的 DSA。大家可以回想什么？其实我们讲的这个 RDMA 网卡， RDMA 网卡本质上是一个网络协议的 DSA，
49:22对不对？就是因为你这个协议已经非常稳定了，大家都在用了，所以我才得什么，我得搞个 DDAA，就把它性能做到最好。
49:29但是我们其实在市面上看到的这个其他的 DSA 的例子相对来说是比较少的。因为核心原因就是因为第一应用其实是在变的，然后以及它变的话，就会导致你因为一个场景做的 DSC 啊，它其实很难去泛化。因为你这个 DAC 它它毕竟它是，
49:45我只能做一个功能，这个大家相信应该应该都能理解，对吧？所以大家可以看到的是，即使是英伟达，对吧？
49:54英伟达的话，它的这个这个这个这个里面其实它它虽然有探测廓，但它其实还是保留了很多 CND 的廓。去做这个这个这个这个这个那保留很多 CMD 的库，去做这个这个这个这个这个计算的。因为本质上的原因是因为大模式里面，
50:10它其实不仅仅是或者说 AI 里面，它不仅仅是数据值化，它其实还有那些激活的这些操作，它其实都是需要一些节奏 purpose 这样一个计算啊，要不我们继续上上完，我们待会早点下下下课，
50:23好不好啊，这样不在外边，对吧？所以所以这个其实我们也可以看到的是英伟达一个非常大的优势点。我个人觉得就是大家想这这个其实本质上在在什么，就本质上在于说你一个 GPU 里面对吧？
50:37你的这个 cancer call 和这个扩大扩的配比，扩大扩这边 LU 的传统的 AOCMD 配比到底是多少？这个东西它其实非常依赖于什么依赖于这个 workload 的，就是场景的这个需求的。就是说就是说我如果大模型对吧，它到底是要 cancer l 多，
50:52还是要这个扩大扩多？那么英伟达它为什么大家说这个扩大护城河这个很难破，对吧？其实很大部分原因是英伟达在很长时间内，它积累了很多这个用户的这个数据。所以它的这个 GPU 啊，
51:05它的这个这个 tecircle 和扩大扩的配比。其实非常好的。那大家想想，因为为什么说我们说你跟华为是是自然的对吧？华为的 NPU 大家觉得很难用。那么其实本质上一个一个配比的，
51:15因为它的这个它其实也是 NPU 扩大扩这个这个这个配比的。其实声炮有一点这个配比的问题，就华为就没有英伟达那么多的这个历史的这样的一个数据。它没有办法做一个很好的 data， driven 的这样一个这个这个芯片的价值。所以说所以说其实这个这个事儿啊，
51:32这个事儿其实还是非常非常我觉得还是有非常多的啊这个研究点去可以做的。这点错了， OK，当然这这个是啊有点题外话了。就是我们做系统软件的话，一般来说我们很少会直接设计硬件。
51:44因为设计硬件是个非常非常大的这个这个 topic t 结构，老师会会专门去去研究这个好吧。那么我们系统其实讲，我们只需要知道说 OK 你的这个硬件它有不同的这样的一个配比啊，这些配比其实是有些啊 trade off 的那我们希望是在这些啊 trade off 上去，尽可能的做一个更好的这样一个系统 OK。
52:03好，那么到这边为止呢，我们其实就讲完了一个单机的这样一个流程 OK。那么单单芯片的话，这个流程我们去总结一下的话，我们就会发现一个点就是假设啊我们这个芯片的面积它不去变大，
52:16不会变变大。当然你每一个原件，比如说每个 ALU，你很难再进一步做小小其大，大家可以看到在在 AAUU，你很难很难做小，
52:23因为你做小意味什么，你的制成要要变得更高，对吧？你要变成你一纳米一纳米以下。那这个时候其实料已经到了量子物理的世界。其实你你再做做做做做小的话，
52:34它的代量很大。所以我们可以看到是你一个设备，你不管是堆 cancl pho 也好，堆 VL 也好，它其实是有一个非常大的这个上界的。就是说你其实它每每年可能最多的增长幅度，
52:47也就是一点二倍或者一点三倍。大家想想是英伟达之前宣称的什么两倍的提升，对吧？或者四倍的提升。我们前面说过它本质上什么？它本质上是把一个 AUU 搬了两个用，
52:57就原本一个 ALU 我能算 FE 八的那我可以同时算两个 FFP 四，所以说它存储变两倍了。但是它的其实在单个 ALU 的这个能力上，或者说整整体的这个 AL 数量啊，其实并没有变得特别多。当然它其实还是在逐步变大的，
53:11还在逐步变大，但其实没有变的变大的特别多。那这个时候我们如果要进一步的去增强算力的话，那只能靠什么？就是只能靠我去买很多的这样的一个卡，对吧？
53:23把这个东西给堆起来。那这个就是我们就是说这就是说到了我们的这个分布式的领域。就是说其实你看现在的所有模型，因为你在想我模型要 skill 其实很容易啊。我只要把参数要乘二就行了。但是你的硬件，
53:38你单个芯片它其实算力是没有办法查，但是保存也没有办法乘二，或者说得等个一年，对吧？等个两年才能乘二。所以这个时候我们一个很很好的方式，
53:48就是说我得把不同的卡给堆积在一起。那么你当你把不同的卡堆叠在一起呢，就会遇到一个问题。就是啊我们说你在一个单芯片上编程，我们就要考虑 CCD 已经有考虑这个这个各种毛腿扩啊，还考虑天测库。
54:02但是我这个分布式它其实会它其实遇到的问题跟我们这个多线程其实很像，对吧？你在多线程，我们每台机器到底要算哪些东西，以及我这个多线程的这个机器对吧？怎么到底怎么跑？
54:14以及我这个到底我在不同机上跑任务之间是不是有 dependence，以及我们说这个 for forets，对吧？就是说这个这个这个这个这个这个我到底怎么做容错？一旦你的规模变大，我们说这个容错的开销会变大很多。
54:28包括你在分布式下的话，它的缓存开销就那个 rufflight 那个 stream streaming 的那个斜线，它其实会低很多。为什么？因为你的这个分布式的话，网络即使是你有现在最快的 m 个 link，
54:38它比你的这个单机的 HPN 大概也会慢个慢慢一个数量级，差不多是慢一个数量级。所以说你的通信会变慢。因为说原本我可能数据搬移不是瓶颈的这样一些场景，可能数据搬移就会变异呃变成场景。那我们怎么去把啊这些东西都简化呢？
54:56这个时候其实我们就知道系统里面一个，