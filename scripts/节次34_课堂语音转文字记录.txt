00:28嘘是是是不是往前还是嗯你嗯嗯你是好啊，我们继续 OK，上节课我们讲过了，对吧？就是说上节课我们再有 cap 一下，对吧？
05:40我们讲到说啊，我们的目标是我们需要提供一个 elastic 的这样一个 model service 的这样一个服务。然后实现这个 elect more service 图的一种方法，一种方法是 loadad balance 啊， load balance 的一个问题。就是说我们在做 loadbalance 这个过程中，
05:54对吧？可能有请求的这个图实实会受到影响。那么我们怎么怎么去，刚才不是你讲的，它其实核心的方法就是传统了 OS 的这样的一个贝这样的一个技术。就是说我在做这个迁移的过程中啊，
06:05行动可以继续执行。但是这样的一个并行的执行会带来的一个问题。就是说我们的这个这个并行执行的呃这个这个这个传的这个数据可能会出错。这个怎么办呢？其实本质上是什么，我们就需要 detect 啊这样的一个 GPU 执行过程中啊，
06:19它不管是从框架层也好，从 OS 也好，我们要要 tedetect 到它到到底要访问哪些数据。而这个这个方法呢，其实在 OS 层是可以完全做到不用修改一行代码的。通能西上也好的。
06:29具体做法就是我们可以去猜这个 GPU control 到底访问哪些数据 OK。那这个时候大家可能会肯定会都会指个很很直观想想过问。就如果你是猜这个 GPU，对吧？它访问的这个数据，但你万一猜错了怎么办？
06:42比如说我们猜测这个 GPU 访问的是比零巴克零，然后恰恰好就访问八块一，怎么办呢？这个时候其实我们就需要有一些什么兜底的这样的一个方案。那么这个兜底的方案，我们能不能不改框架不改 GPU 的这个这个呃不改坏框架不改不改应用去做呢？
06:58其实可以做到的。就是现在的这个 GPU 磕南啊，大家想 GPU 的这个刻度，它本质上和 CPU 的这个 process 是没啥区别，它都是一堆白 OK。那么这个的话，
07:07我们其实就可以用这个这个经典的一个 OS，就叫做 binary instruramentation。就是我们可以对于这个啊 GPU clino 的代码去做动态的这个插装。通过插装之后呢，我们就可以去插入这个检查的这个代码啊，去看我们这个猜的对不对啊，
07:20这个其实是可以离线做的，离线做的。当然这个时候大家可能会想一件事情，就是说你既然能 GGUU 插装，对吧？为什么我们不直接就用插装的方式，
07:29然后让这个 GPU 去把它的 output 的这个 dream 给输出出来了，而是还是还要拆过呢？其实是一个啊，其实其实是有有有做这个考虑啊，大家有兴兴去看，我们配合基本质上的原因，
07:41就是说我们去验证啊，我猜的对不对？这个事儿比我去直接输出我的这个计算有这个事要要更加高效一点。因为我验证对不对的，只需要输出我每个插装，我只要输出一个一个 value 就可以了。
07:52我只要去输出这个对来是不对。但是如果我要去这个这个这个这个这个我要去把它这个整个 vofflow 出上，我得去做这个 GPU 的这样的一个 q 的这种分配啊，一系列的这样一个操作。这个东西其实是啊相对来说会拆的更大一点啊，这是一个原因。
08:06还有一个原因就是说猜它有一个好处，它是可以在这个 GPU 执行刻度前，我就知道它的这个数据流了。但是你这个基于纯插装的方法的话，你其实只只能等这个刻诺结束了，我才能知道这个数据流。
08:17那么这种的话，在这个 my gration 的 case 下，可能其实是不不不重要的。但是在一些其他的 case，比如说你在做这个 online store 对吧？应在做这个 check point 的纯嵌框里面。
08:26 ccase。其实其实是是是需要这个提前知道 OK。所以在这种情况下的话，我们是是是是需要还是需要猜这个方法 OK。那么有这种方法学之后呢，哎我们其实去做这个这个买贵线就非常简单了啊，
08:39这个这个就跟传统的这个一样了。 OK。那这个时候的话，其实我们就就啊就是就是说我们可以可以会发现啊，相比较现有的这样的一个这个方案。比如说像微软的这个 americactive，
08:51包括易伟达这种扩大切换，就是呃扩大切换的。其实就是我们能够啊相比较把这个迁移的这个时间啊，可以做到非常非常非常非常短啊，相对来说非常非常短，就停机的这样的一个时间 OK 行。
09:02那大家感兴趣的话，可以看看我们这个 OK 行。那在这个又是好，我们前面讲的是第一个方案啊，第一个方案。第一个方案就是说我们通过这个 load balance 啊，
09:11 load balance 这种方啊， load balance 这种方式，去让这个一个 service，一个 service 的这个这个这个这个这个这个这个这个更加均衡。但是这个 loadbalance 这套方案呢，对于一些小模型，
09:23比如说你是这种七 d 的啊，或者是三 d 啊，这种模型其实可以 work。因为它有这样一个 corlocacation 的空间。但是对于一些大的这样的一个模型，对吧？
09:33就是尤其是这种企业的模型，这种设计其实很难做。比如说我们可以看到的是当这个这个这个对吧？大家想想就传上面这个图，其实就是传统的 call service 的这样一个 corlocation 的 pattern，就是我不同的 service a 和 service b 他们其实非去卸一些 CPU call 去做超卖。
09:49它本质上的原因是因为我的一个 CPU 的资源，它能够把你这个 service a 和 sers b 的这个 working 色彩全都给给给放进去。但是呢如果你是一个 GPU 的服务，大家可以看到一点，就是说现在你的一些旗舰模型，对吧？
10:01我我一整台服务器中的这个这个 GPU 都给你啊，它还不够。那你其实就根本就不存在这样的一个超卖的空间，超卖的空间。那这个时候怎么办呢？怎么办呢？
10:12就是这是实际上是这个 colocation 它买过去它它其实没有办法解决的问题。买微去它能腾出的这样的一个计算资源，还是非常非常有限。这第一点第二点就是说我们前面做这个克鲁推析的一个强的假设是什么呢？就是说好，一旦我这个 service 它 load 爆炸了。
10:29好，我把这台机器的服务迁走了，迁走了，其实是能够有效的这个这个服务的。但是如果会不会有一种情况，就是我这个 service 它它它提高了，
10:39提高了非常非常多多到什么呢？就是我即使把这台机上所有的服务都清空了也做不了，这个候怎么办？其实这种时候我们其实就需要什么，我们就需要额外的找一台空的机器，对吧？
10:51去把这个模型再给部署上去部署上去。那基这两个原因的话，那就就就就得引出我们的这样的一个第二套方法，叫做 auto skilling。那 auto skilling 它本质上不仅仅我们前面说的要有 skill up，它其实还有什么要 sill ll down，
11:06因为你得去腾出一些资源，给那个叫什么给这样的一个模型 OK。那具体来说的话， auto skill 呢它这个方式是这样的。就比如说我的系统里面，它有一个这个 monleor，
11:15它会监控你的这个整个的这个 load 啊、蓝色的啊、蓝啊、红色是用户的这个请求漏 ad。然后蓝色的呢是我们部目前部署的这样的一个 GPU 的资源，为这个服务。
11:25好，当我发现这个这个你的有快速的上升手怎么办？这个时候我就需要去找一些空的这个 GPU，把这个我的这个服务 instance 给提起来。这样的话我能够把我这个蓝色的这个 serving capacity 给增加，对吧？
11:41从而提升这个系统的性能。然后光有 cappasacity 增加是不够，为什么？因为你一旦增加之后，我所有的服务都增加，那我这个最终 GPU 全都用完了，
11:49那我就不就找不到空的 GPU on 了。所以呢我们还需要是吧？当你的这个流量下下来的时候，我要把是吧我要把你这个 service 关掉关掉啊。 OK。那这个所以说它其实包含了两步，
12:00一部分叫 skill up，就是我需要去部署更多的服务。另一个我就是 skill up 定期的去把服务服务务掉掉。但是这种模式呢，其实其实现在的这个云厂商啊，就是说尤其是卖 APF 服务的啊，
12:13大部分还是会采用这样一种模式。因为这个模式它有一个非常好的特点，就是我我去卖用户，对吧？我其实不用卖这个 GPU。大家想其实现在云厂商要卖 APF，
12:23他卖的什么？他卖是 token，就是你到底处于多少 token。这样的话，我的云厂商只需要保证什么？就是我的这个 token 的产生的速率，
12:30在一定的这个这个符合用户预期的条件内 OK 哪怕这个模型它到底用多少台集群，都是我我这个云厂商决定的。我就可以把我的这个集群啊尽可能的去服务更多的这个模型，对吧？来获得更大的这这个利润。 OK。
12:43好，那这个就是奥特斯格理好，那 auto skkilling 有什么挑战呢？是吧？这个方法听上去很 work，对吧？
12:51那么这个方法有 work 的一个核心的前提是什么？大家想想，如果你是 skill down 啊， sskw down 这个东西其实很方便的，对不对？那核心前提是说我一旦用户这个请求啊上涨了，
13:02我这个 skill up 得非常快的 skill 上去，对不对？又有什么？一旦我用户，比如说他的请求是这个蓝色的虚线上涨，但是呢我的这个啊不对，
13:10这红色上涨。然后呢，但是我如果我要 skill up 的这个时间很长很长，比如说我要花那么久的时间去 skill up，那这个不就会导致是说我在 skill up 的这段期间内仍然有很多请求在排队。那这个不是请求还是会会很长嘛。
13:25那这个其实就是在模型 skill up 的这样的一个一个一个 data space 里面，它的一个最大的这样的一个挑战，对吧？大家如果有起过这种开拓 ch 的话，就可以可以体验一下。说如果在某个语音厂商，
13:38你去租一台 GPU，然后你第一时间第一次提一个拍拓 ch，对吧？大家是否都知道这个东西大概至少至少需要几分钟才能提起来， OK 这个东西怎么办呢？其实那我们就得看，
13:50对吧？这个 scill e up 它到底能做到什么样的程度，对吧？我们们其实希望就是是说它一定要把这个 skill up 的这个 lency 啊啊给降到最低降到最低。那么 skill up 它到底包含几步呢？其实我们可以看到它其实包含很多步。
14:03然后有一些步骤呢，它其实是是是可以用传统的系统方法解决。但是有一些是是这个模型这个独有的这样艺术。那具体来说的话，其实我们可以去拆解一下啊，整个个拍拍的的，
14:16比如说我启动一个个 DM，对吧？它其实本质上是要分成三步。第一步就是说我先要把这个拍送的这个轮胎启动化轮胎启动好之后呢，我还要去创建一个 GPU 的执行环境。就我要把这个 GPU 的科呢啊对吧？
14:31给给给到到 log PU 上，我需要去设置 GPU 上的这个页表。然后第最后呢我需要去什么把这个模型给加载上去，加载上去。其实每一步啊，它其实都会有这样的一个这个这个这个这个比较大的这样的一个开销。
14:46但是比较幸运的是，就是说啊其实每一步我们都是有一些比较明确的这个系统的技术去可以可以可以解决的啊，可以解决的。好，那么我们今天先看第一个吧，就是这个这个这个这个这个这个这个加载参数的这样一个开销 OK。
15:01好，就是大家想想看，有没有如果我们第一次对吧？部署一个服务。比如说我们第一次在这个这个这个机器上，我们要跑一个模型，
15:10那我们要干的是什么？我们得从这个 hohoge face，对吧？这样的一个这个这个这个这个这个这个地方去下载模型，那么先不考虑对吧？这个这个你的这个 hoge face 有没有被强或者啥问题，
15:22对吧？你就就假设我即使是没有墙，我用了一些比较好的这个镜像，它的这个下载速度其实 btleg 在什么？其实在 borleg，在你的这个 storage 到这个这个这个这个这个机器的这样的一个网络注宿，
15:35对吧？比如说你是一个这个这个十三 b 的模型的话，它其实大概就是有二十六 GB 的这样一个大小。那么我如果是用一个五 GB 的，这样网速其实已经很快了，对吧？
15:44五 GB 五八四十吧，四十个小 g 对吧？其实已经是个不是很慢的这样的一个网速了。那其实仍然需要这个非常长的时间。那么它的这个这个这个这个这个本身它其实比这个 TTFT 和 TPT 其实都都会大很多的 OK 好。恶心是行，
16:07那么 OK。那么那么这个东西它其实除了你下载这个模型参数，对吧？其实我们说如果我们站我们要站在的是一个 call service prider 这样的一个角度，对吧？比如说我一个啊 service call prider，
16:19我要比如说我需要 skill 一个容器，对吧？这个容器要要要去这个这个这个这个这个这个服务，这个这个这个这个 VM 一，比如说这个这样一个谁，那大家想想我们的这个 call prider 干的是什么？
16:32干的事情，其实本身它还需要先做什么？现在再跑一个，包括论就是说他需要把这个容器跑起来。那这个 doctor 呢它又会第一步啊，先把我们是吧先把我们的这个这个这个这个这个这个镜像啊，
16:45从这个这个 dockhelp 去下载下来。然后呢我们要做一个什么？它说一般来说要做一个就是 conconttalization 的这个操作啊，其实原因是因为你这种 code，它要保证这个不同容器之间是有这个隔离性隔离性的。所以呢我得去创建一个容器的这个具体的隔离环境。
16:59其实本质上就是你要去设 c group 和这个 news space。然后最后对吧？我看我还要去初始化一个这个这个拍上的这样的一个这个轮胎啊，初始完完后 OK 我才能去啊做这样的一个操作。那这个操作的话，其实我们可以看到是啊它的前三步啊其实都是有非常大的这样的一个开销的。
17:19当然这前三步的话，它目前的话就是说它有一些有一些你比较具体的拍摄，你们可以解决。比如说你这个容器的镜像下载很慢，对吧？在这个时候的话，
17:29我们其实可现是属是这个基本上问题已经解决差不多什么意？就是你可以用用一些比较快的网络，就我们之前讲的这个 RDA 去做。所以说这个器器这个事儿本身是可以做很快的。那么你至于这个 conttaization 呢，这个事情其实在这个这个这个操作系统这边，
17:44对吧？它其实研究了好多年，他们有一些有一些比较经典的，就是说叫做这个池化器。所以思就是说我可以提前去建立好一些这个模板容器，就是这个容器啊，
17:54它它已经有这个比较比较好的这个这个 conttaization 的这个 framework 了。所以说啊我去做这个这个这个这个这个这个这个这个这个这个这这个容器化其实是非常非常好的。那么其实现在现在容器启动其实一个非常大的问题，就是我怎么去避免这样的一个拍照能判的这个时间。因为这个对于其实对于这个 AI 还非非常重要。因为传统统的应用可能 CI 写的，
18:17它这个启动非常蛮快。但如果你是一个基于 python 或者 go，对吧？或者 java 写的这样的一个应用，它的这个时间期就非常长了。那这个时候这个问题怎么解决呢？
18:26其实这个问题我们可以再再做一个拆解啊，其实我们想做系统研究的那就是就是对不同的这个技术做一点点拆解嘛。其实我们可以看到 python 它这个轮探的这个这个初始化过程，不知道大家有没考虑过它到底怎么执行，对吧？它这里面的执行其实本质上就是做三步事情。
18:43第一步的事情呢，它就是把 python 的这个 module 啊，就是我们 import 那些东西被 import 到这样的一个 python 这个地术空间内啊，这是第一步。第二步事情呢就是把里面的一些关键的函数，把它变成这这样的一个这个 bad code。
18:57那照 python 它是实际上是一个解释执行的这样的一个点，对吧？它那你你它它代码是不能直接跑的啊，如果你你你啊对不能跟能接接行，就把它做一个翻译。最后呢我们每一个 module 上，
19:08它其实是是有自己的一些这个 initialization 的这样的一个这个模块，我们得去做一些这样的一个初始。那这些操作它导致了一个结果，就是说你的这个整个这样的一个这个这个容器啊的初始化就非常非常慢慢。那么这个事情怎么解决呢？其实我们稍微观察一下的话，
19:25我就会翻一一个函数也好，就我不管是漏 ad 这个 python modul 也好，或者说我是 transsit bad code 也好，或者我去执行一个函数也好，它其实本质上都相当于本质上，它都是相当于把这个拍放的这个容器啊，
19:37从一个状态去转移到另一个状态，对吧？这个其实是这个这个之前那个呃南大那个小莹燕老师吧也讲过同学的一次观点，就是说 everything 也是 statement，对吧？就是我们现在的这个所有程序，
19:48其实都可以看成一个 statement acate。那所谓的 statement ine 就是什么意思呢？就是说我做的任何的这样一个计算操作，我都可以被认为是从一个 state 转移到另一个 state OK。那所以我的一年下来一起起了那么久的时间，我其实本质上干什么？
20:01就是把从一个初始的 state 转移到一个 initialize 后的这样一个 sit。好，那么有了这样一个概念之后，假设我们分号，我们能有了一个这个 unitionalize 后的这样一个内存状态。大家想计算机的状态本质上都存在哪？
20:16就是你的内存和文件，对不对？那有了这样的一个初始化好的内存状态和这个文件之后，好，我把大家一起打个包。打个包之后，
20:24如果我的一个程序从这个打完包的这样的一个里面去启动。那这个事情我不就可以完全去跳过前面那么复杂的这样的初始方案吗？这个其实就是现在这个优化容器启动的一个非常经典的思想叫做什么？叫其实就是我是要给一个初始化老实话，打个切货的。然后再从这个里面去说 OK，
20:44这就是啊优化这个东西的基本的这个方法。但这个东西它有一个问题，有一个问题呢，就是比如说我要去启动一个容器，对吧？那我其实要启动多少个容器，
20:56它这个东西实际上是非常不确定的。什么意思？就是我我理论上来说，我们说在刚刚那个 auto scaling 的场景，对吧？我要启动多少个容器，
21:04它 depends on 什么？它去 depends on 你到底需要有多少个容器才能提供这样的一个计算能力，对不对？那其实是理论上来说，我们最最理论上来说是什么？我最坏情况我可能是有 n 个容器要去做这样的一个并行的这样的一个启动。
21:18并行启动，那意味着什么？意味说如果我要从这样一个这个这个个打包的这样的一个方法打包这这的一一个方法啊，去这这的一一启启法，用什么？就我如果要要用用一个容器，
21:28我我就什么么在 n 台台器上都都有这这样的一个打包好的这样一响。那这种方法的话话虽虽然够够做，很快快的加速，但是它有个问题就是啊我如果这个这个这个 n 设的不对怎么办？设设多，比如说我 n 设的非常小，
21:43那如果我一个启动的这个数量远大于你这个 n 你就不不行呢，就就爆掉了。然后那大家很好奇的问，为什么我为什么不能设的很大呢？第一，这个 n 大家想想这个 n 本质上取决于什么？
21:55取决于就是说你的这个机器上要部署多少个这样的一个打包好的这样的一个这个这个这个 state，对不对？那么你想想看，我存这个 state 其实需要花额外的这样的一个开销的这个开销。在这个 AWS 就是亚马逊云上，它叫一个东西，
22:08叫做什么叫 promotion concurrency。就是说你得你要做加速，可以，你得提一些什么部署好这些镜像，那你不给部署一份镜像，我就得什么，
22:17我就给你给你收费。为什么？因为这个云平台厂商，它的这个我每个内存比特他都要卖钱的，对不对？你用了我那么多内存资源，
22:25那我肯定要给你收费。那这个其实是是一个非常不好的不好的这个事情。所以说这里面它其实欺负这个东西的难点在于什么呢？在于说我能不能啊只有一份这个打包好的镜像，然后我却能够达到这样一个启动 n 份的这样的一个数据数数据。那么这个时候啊，
22:45这个时候啊，其实我们去看一下现有的方法，其实你就会啊基本上发现现有的方法是都都是都是不大的 work 啊，都不大的 work。但这个这些都是不不是很那个都是 OK。所以说这个时候呢，
22:57我们就这个时候我们提我们就提出了一个新的方法，对吧？这个方法叫啥？来叫做 remote folk 啊， remove fork，我叫 remove fork 呢是大家啊，
23:04我们操作系统里面我有我有我有什么方法能够从一个这个镜像里面，我去一个镜程里面，我们去搞出好多个镜程，就不就是 folk 嘛，我可以一直 folk 出去，对不对？
23:14那么 folk 它唯一的问题是什么？它唯一问题是说它只能在一台机器去 work，就比如说我要 folk folk 到另一台机器这事是搞不定的那其实如果我们有一个方法对吧？我们比如说提供一个类似于和 folk 这个能力，我可以从任意一台机器去打去 folk 这样一个事儿。那我这个事情不就能够实现一台这个一个镜像，
23:34对吧？我可以给所有的这个 class level 用。对吧？这个其实就是啊我们这个 remove folk 的一个思想 OK。那么 remove folk 这个概念看看看上去很简单，对吧？
23:43但是我们说你要去实现它其实实是比较困难，什么意思呢？大想现在你怎么去实现 remove folk，其实就是是们之之前讲过的 check on 和 resort d 这样的一个方法就是。好，我要这个要做 folk 了。
23:55行，那我就先什么先把你的这个状态我打一个切个框子。打完切个框子之后呢，我再通把它存到一个分数为小，或者是用一个网络传入网络。然后呢，
24:04比如说我这个用户他要这个 folk 的时候，好，我去把这个文件从文件系统里面读出来，然后我再去恢复啊这个开销啊。因为这个这个文件系统的开销其实是非常非常大的。所以我们就会发现一个问题，
24:19就是当你的这个这个这个容器啊拍上去容器，它的这个镜像越来越大的时候，你的这个这个文件系统的这个开销其实会完全超过你的这样的一个这样的一个这个这个这个不去做拍放初始化所带来的这样的一个开销开销。那么我们有没有办法能够把当中这一层 CRU 和文件系统带来的这个中间层给干掉呢？这个时候其实我们就发现一个时代，实也不是法典嘛。
24:44就是说大家其实可以观察的一个事情，就是你现在的这种数据中心对吧？它其实都是有 RDV 的 RDVV 什么意思啊？就是我一台机器可以非常快的去独立一台机器的这个内存，对不对？那么其实对于我们这样一 remomote k case 其实也是一样的。
24:59就传统我要做一个 remote folk 的话，是吧？我得用一个 TCPIP，对吧？去把你的这个书镜像全读回来啊，然后再做啊，
25:08这这个网络开上是非常非常大的开额外的背。但是有 cor DMA 之后，对吧？我假设我这个 chard 的，我需要你这个它的这个内存，那我直接什么我直接用 RDA 去把你的这个内存地址给读回来就行了。
25:21那这个事情它其实通过一些这个这个科 o level 的这个就是内核层的这样一个那个那个那个那修改啊，其实是是是是可以可以做到是做到。所以呢我们这边就是说搞了一个事情嘛，叫叫什么它是他干的是什么呢？就说好。假设啊我的这个 child，
25:38然后需要我去从这个 parent 这边拿 folk 呃 fork 一个进程。那么我们的这个 fork 的过程跟一个 local fork，其实是啊一样的，就是我第一步我先去发一个 RPC，把我把你的这个页表啊就是页表给贝过来。贝过来之后呢，
25:52对于每个页表的 calcalentry 之后啊，我们就可以直接用 RDMA 去把它这个 on demand 去读回来。那么这个开销的话，其实相比较相比较这个这个现有的这样一个这个这个这个这个这个 CRU 的这样的一个实现，对吧？它其实就就快了这样的一个这样的非常多。
26:09而且它其实能够能够这个以一个非常资源少的这样的一个形式啊，去去去去实现这样的一个这个这个这个这个这个这个这个联盟 folk 的这样的一个这样的一个东啊这这样一个个 creation OK。但有了这样的一个技术之后，对吧？当然其实我们就可以看到，在这个这个这个不管是你的这这首先是个 face，
26:32就是这个 request rate 的变化。然后这边是我们这个资源占优的这样的一个情况。然后这边是整体的端到端的这样一个启动的延迟。其实我们就可以看到啊，我们在更少的这样的一个这个这个启动延迟的呃不更少的资源占用的情况下，对吧？
26:46 v 一的这样一个情况下啊，其实是能达到这样一个更快的这样的延迟的 OKOK。那这样的话就能对一些经典的容。不管你是从这个呃这个传统容器也好，还是这些有 AI 的容器也好，其实都能获得一个比较大的提升。
26:59 OK 请进行。那么到这边为止，其实我们就能发现一件事情，就是啊左边这一块啊这一块这个这个这个这个容器启动时间其实通常是是能够干掉的啊，能够干掉。当然其实现在还有一个，
27:20那么干掉这个第二个时间，对吧？那么其实我们就需要看第二块对吧？就是说你的这个这个 GPU 的 context ter，就是说我这个驱动创建啊这个事情这个事情就干了啊，这个事情其实我们发现要干掉它，
27:33其实也也也挺简单的啊，挺简单。所以在这我们有一个观察，就是你虽然很多的这个 AI 应用非常多，对不对？那这些 AI 应用它基本上都是这个 pitorch 的这个 eco system，
27:43所以呢它用的这个 GPU 的这个 context 啊，其实是差不差不差上。所以呢大家想想，其实原本我们比如说要启动一个应用，对吧？我得先把这个这个这个这个这个创建一个这个故障的 context。
27:57好，创建完之后呢，我才能去跑这个 t 就是这个 GPU 的啊这这个创建 context，然后才能去跑这个 GPU 的 conal。但是如果我们有一些公用的这个 context，他已经创建好了啊，
28:08其实就就是我们前面讲的这个 puling 这句话。那我如果用户他创建时候，我们直接让他使用一些创建好的这样的一个 context。那我用户他只他下一他他再一次启动起来时候，其实我们就可以去什么避免掉从头啊从头去啊创建这样的一个 context 的开销啊，这就这就这是一个非常简单的这样一优化。
28:30那这个优化它用做起来的话，其实会有一些有些挑战战是说你你得保证什么呢？就是你得保证这个用户他不不修改，对吧？你去实现这个东西。因为啊比如说有一些有一些去实现这样的一套方法。
28:40比如说像 VM 最近提的这个 sleep 吧， CPU 那些 model，其实它本质上什么，其实是你你只能在这个 VM 这个 sleep 的这个框架下啊去使用。但是如果你是一个任意的这样的一个 GPU 的这个应用啊，它其实就就做不到啊这样的一个 context 这样的事情。
28:55所以呢当时我们其干了一个事情，就是说我们做了这个这个这个这个 GPU 的这个虚拟化啊，就是说我们其实会结持这个用户的所有的这个 GPI 的呃 GPU 的这个 API 的这样的一个调用。那这样的话用户调一个 API 的时候，我们去给他导向这个我们提前创建好的这个 contest 啊，其实就能够避免掉很多的这样其实用户去不需要修改啊，
29:15就能够去避免 context。这样的话 OK 然后有了这样一句话，再加上我们前面啊说的这样的一个这个容器启动的这样的一句，对吧？其实我们就能够啊做到一个这个这个这个这个非常快的这样的一个这个这个这个这个这个这个 GPU 的启动了 OKOK。好，
29:32那么有了这这样前两两优优化功呢，其实我们就可以干干掉两件事儿，对吧？第一个事儿就是说啊我们我们把这个这个这个这个这个这个这个 GPU 的这个啊这个这容器啊启动全干掉。第二个呢就是我们把这个 GPU 的这个 contemption 干掉。当然这这这其实两块有些其他方法啊去干，
29:49我们这边并不会发。那其实其实大家可以可以进去看一下，但是他们的核心的思想其实都是都差不多的 OK。那么最后一个事情的话，其实就是说我们怎么去啊加速 model loading，对吧？
30:00这个 model loading 这个事情就是啊这个我怎么怎么快速的去把这个这个这个这个这个参数啊给加载到到加载到 GPU 上。这个事情其实是是是相对来说是挺花时间的。我们前面也说过， OK。好，那么大家可以我们来思考一下这个问题，
30:16就是 model 逻辑，它和前面的这个 GPU context 也好，和这个这个这个这个这个这个容器也好，它稍微不大一样。它不大一样的点在哪呢？就是说前面的 context 也好，
30:27包括我是也好，它更大的问题其实是现有的这个 conttasystem mechanism。比如说我这个 OS 的这个容器，对吧？我的就是 driver，它对于你这个启动啊非常不友好。
30:37但是 model loading 这个事情呢，它有点受制于这个物理有限的。所以参加想啊我们的 model 的这样的一个参 loading，它本上什么就是我要把你的这个 q 呃 momodel 的这个参数啊去落到几句上啊， model 的参数它其实是非常大的那对于这样一个非常大的这样的一个这个传输的话，那么它的这个核心的这个时间取取决什么？
30:58其实去取决的就是你的这个带宽，对吧？你的带宽能不能足够快的这样的一个传过来。那么我们之前讨论的，包括大家现在其实如果你不你不是站在一个呃 model service provider 的角度来说，我们其实用的最多的什么？
31:11是这个 cause storage 是吧？就是说我是这个云厂商，那云厂商呢啊不不是云云存储或者是 hogfface。但是 hube face 这种呢，它的带宽其实是很低的。那么我们能不能比如说我站在一个云服务器厂商角度来说，
31:24我能不能把这个模型参数是放在那个离 GPU 这个这个这个这个这个这个这带宽最大的这个地方呢？那其实这个就涉及到一个点，就是我们说的这个 oud，就是 data senten 里面它其实是有一个 call storage 是吧？就比如说最上层啊，我如果是把这个参数直接存在 GPU 里，
31:41那它的这个带宽其实最大的，因为我可以瞬间加载完。但如果我 GPU 没有这个模型参数怎么办？我就得用什么？得从 CPU 去加载 CPU 的。它的带宽大概是两百到到三百啊，
31:52不不不不两百百，剩上个这个什么二十五 GBPS 到五十 GPS depends 你的这个 PCIE 的这个版本。然后如果 CPU 存不下怎么办？我们只能去存这个 SSD。对吧那 SSD 就会显著的到很多，它其实并不一会会快很多。
32:06所以我们第一个能想的一个事情，就是我们能不能有没有可能我们我们把这个 model 啊它放在这个 GPU 和 CPU 里。就是我我们把这个厂商它服务的这个 model 全部放在最快的这个 storage 层次，可不可以呢？其实这个东西显然是不可以不可以的，为什么？
32:21因为我们其实可以看到你的一个 storage，还 erapy 啊，它去获得提速的一个代价是它的这个容量会逐层变小。那你越慢的存储，它的储容量越大越快，存储容量越小。
32:32那么你做一个厂商，如果你是一个厂商，你的只用 CPU 和 GPU 的话啊，其实我们可以看到是你的 per sover 的这个 GPU 和 PUU，内容量是非常非常小的。这流量它是实际上是没有办法去支撑。
32:46就是说我每一次启动都能够从一个最快的 storage 去层啊，为什么？原因很简单，我们前面讲了，你我们是一个 model service 的，就我们的场景啊是一个 model service 的这样一个 provider，
32:55对吧？你一个 provider，你不是只只服务一个模型，对吧？你要服务的是几百上千个这样一个模型。你要保证我这个几百上千模型，
33:02我每次启动的时候都能够拉到这个事情。其实很难的那这个我们其实当时也做了一个具体的这个实现，对吧？比如说我们把现有的最优的这样一个系统对吧，我们把它跑在一个真实的场景上。然后我们用这样的一个全局的这样一个模型的缓存管理，
33:17对吧？我们就会发现啊，确实我们能获得加速。比如说我们有百分之四十的这个启动，我们所有从 GPU 或者是从 CPU 那这样一个启动了。那我们能不能那那但是其实问题就是你还是有百分之六十的这个启动，
33:30对吧？是没有办法从这个啊 model cash 里面去加速的 OK model cash 加速的。好，那么这个就是一个第一个问题。那第道的问题啊，就我们能不能用第二层快存，
33:39是因为我们说这个东西其实本质上来说什么，其实在于你要选择这个最快的这样的存储资源，对吧？那么能不能选择第二层呢？其实像现有的这个工作，比如说像 solo RM，
33:49对吧？它其实用的就是这样的一个这个这个 SSD 啊 SSD。那么 SSD 它有什么好处？就是我们可以看到这 SD，虽然它看上去只有三到七 GBPS，对吧？
33:58但是其实我们可以在一台服务器上塞很多 SSD。我们以一种这个 read 的这种方式啊，去把这个 SSD 整体带宽去打上来的。好，那么我们 SSD 能不能 work 呢？这个时候我们就去 soll s proud，
34:12对吧？我们去看了一圈啊，发现一个很有意思的现象，就是啊虽然我的这个 SSD 啊能够提供很大的这样的一个带宽。通过 read，但是现有的厂商它其实都没有采用 SSD 这样的一套方案来做。
34:26比如说以睿智话来说说，比如说我们可以看到的是，不管是你是阿里啊也好，还是谷歌也好，还是 a ws 它的这个 GPU 服务器的这个 SSD 本地 SSC 带宽都配的非常小，为什么呢？
34:38这个其实是得从一个这个 code service provider 这角来说。大家想如果我是 code service provider，然后我也有很多服务器，这个服务器的每个服务器组件。你大家想对他来说，它是组件应该是越多越好还是越少越好？
34:50它其实应该是越少越好。就我一个服务器的，它不应该有太多的这个拷核的。因为不然的话，比如说我一个服务器塞了八块 SSD，八块 GPU，
34:56我 SSD 坏了，我就得什么我的这个修修理工对吧？我就得去把这个服务器给拆开来，把这个 rock 给推出来去修。这个维护成本其实非常大的。尤其你这种啊，
35:06你这种 SSD 它可能还有坏盘的这样的一种这种情况的话。所以呢其实现在云服务厂它其实不都都不推荐使用 SSD。他们推荐都采用了一个什么叫做这个 dissaggating 的架构。我不知道大家有没有听过，就是说我的这个服务器是 SSD lest 就是我直接通过网络去访问一个专门的 dedicated 的这样一个存储的 SSD 集群。这个其实在国内，
35:27就是现在华为云的这样的一个数据中心，其实已经是实现了这样一种呃 SSD list。那这种好方式的好处其实是它的这个每天 list 啊，就是它的管理其实是非常非常方便。但它的代价就是啊它的没有一个 SSD 作为一个加速了。所以说你其实是没有办法像 SECM 这样一个这样用这种 SSD 的这种方式去加速模型参数加载的 OK。
35:50那么我们干了什么事儿呢？我们干的事情是非常简单，就是说呃我们发现你的这个既然你 SSD 不行，那我们看看这个这个数据中心对吧？还有没有啊，快速的这样的一个存储介质。
36:01我们发现有就是现有的这样的一个 service 的 class a 它的机器之间的网络其实都是很快的。就是它用的都就是我们前面提的这样的一个 RD 杯 RD 杯。好，那么假设我有一个模型，对吧？我存在了一台机器的这个急救中，
36:19那如果我要 skill 的时候，我从这这台机器啊直接去通过 RDV 把这个参数给拉过来。那这样的话，我不就能够去实现一个这种非常快的这样的这样的一个这个这个这个速度嘛。好 OK，那这个其势就是一个基本的这样一个思想。
36:36其实我也看到的是啊不不同的这个数据中心啊，数据中心啊，它其实是也它的网速其实都是远大。你的这个 SSD 的这这个配置 OK。好，这是一个看上去很经典的这样一个设计，
36:50对不对？但是这个设计它有什么问题，它有一个什么问题呢？它其实有有很很多很多需要做的点。比如说第一个点啊，我发现就是说你如果我这个模型对吧？
37:00我只要 skill 一次， scale 一次，那这个问题是很简单，我直接网速拉过来就行了。那如果我这个模型我要 scale 好多份，比如说我要 skill 五六份怎么办？
37:10比如说我原本啊我现在是从这个机器里面对吧？ skill 一次，它的这个时间其实约等于是个 m 就是模型大小除以这个 b 就是 banues。那如果我要 skill e 两份，那我不就变了了倍倍的这个时间了。那那它你随着你的 skill 的份数越多，
37:24那个叫什么？我的这个这个这个模型的这个这个其实性 skill up 的这个信息越来越差，对吧？那么你 SSD 有没有这个问题呢？ SSD 它有个好处，什么 SSD 它容量是很大的，
37:35容量大意味着什么？意味着说我可以每台机器，我都可以把所有的模型参数都存下。那这样的话我其实就是有 n 台机器 LSQ 的话，我直接就有 n 个 n 倍的这样一个带宽。但是如果你是网络的话，
37:46我们你只能用什么整个 CPU 和 GPU 的这个内存。那你不可能说我 CPUGPU 内存把所有的这个模型都存一遍，对不对？那这就是一个问题，我们能不能做到？就是说我假设啊我这个这个这平台里面只有一份或者多少份参数，
37:59但是我能够做一个这个 skill 的这个时间，它是不随着 n 去改变呢。这个时候我们其实就会发现，在这个传统的网络里面有一个非常经典的操作，叫做 multicast。 multicuast。
38:10什么意思呢？就是说对于一个 bad with intensive 的这样的一个操作来说，我们通过一个类似于 rin 的这样的一个 broadcast 的方法呢，是能够实现我去扩 n 个我传 n 份参数跟传一个参数的时间是一样的。这个怎么做到呢？其实我们可以明显的看到，
38:25就是你一个模型啊，它的这个参数其实就是一层一层，对吧？就是每一层 OK。那么假设我们把这个层切的非常常细，切成非常小的这个这个个框。
38:35然后呢，我去做模，我假设我扩两台机器，然后我先第一层扩容到这个这个台机器。然后扩容之之后呢，我立即去把它的这个这个层啊去的给下一台机器。
38:45然后整个这个过程我们把它去做一个拍卖起来啊，拍卖起来。其实我们可以看到的是原本对吧？你传这个第二层以后的这样的所有的时间，它其实都是被 forwoverlap 在起来在一起了。那么其实这个方法它本质是什么？
38:59它本质上就是说你把这个这个整个的这个机器之间的所有的网络它全部都用起来，用起来之后 OK。那这个其实我们就能够做到什么就能够做到。你只需要一份的这样的一个参数，就能实现这个 n 份扩容的这样的一个位置实验啊，这是一个这是一个非常啊这这是一个非呃非常重要的这个这个设计啊。
39:18其实这个设计基本上啊现在很多厂商基本上都是也是啊跟跟我们或者说做差不多的这样的设计 OK。那这个设计的话，其实光有那个叫什么？这个这个设计其实不大够啊，不搭够，它其实有两个比较大的上去啊。
39:30我们说你任何脆弱的设计的吧，其实一般都会有一些啊拆的比较处理。第一个我们遇到的这样的一个问题是说，就是你大家想想我原本一个摩托车 surpride，对吧？我机器期间它有很快的这个 RDV 网络。
39:47好，这个 RDV 网络它其实不是放在那边没事干的对吧？它其实是原本给你做这个 serving 服务的。比如说像现在大部数一推退对吧？有个非常重要的点，叫做这个 PD 分离，
39:57对吧？但是那所谓的 PD 分离，就是说我一个节点，对吧？它要把它的 KB cathe 通过网络传到另一台节点。那这个这个网络它其实得得得什么得得用 RDV 来传。
40:11好，那么假设啊我们要扩容一个节点，然后这个节点它原比比如说正好它在传这个 KP cash，那不就会导致一个结结果。就是我这两个东西打架了嘛，打架了嘛。
40:22那一旦打架了，这个打架了之后，我们这个这个不就会既会影响到现有模型的这个 service performance，也会影响到你这个扩容的这个性能。对所以这个意味着说我们怎么去 basement，我们这样的一个模型参数，
40:35拉取的这个点其实是非常非常重要。这是需要去做一个设计 OK。那这里怎么去解决这样的一个问题呢？其实我们就是也是发现了一些解释，也是不是发现吧。就基于这个 RDV 的一些特性，
40:47就是说现在的 RDP 网络它很有意思，它实际上是一个叫双工的网络。什么叫双工呢？就是说它就是它的网卡，虽然标的是两百 GBPS，但其实就是它是可以达到四百 GBPS。
40:58为什么呢？因为它的这个进的带宽和出的带宽，它是走不同的物理线路。所以我们这个网卡它可以同时做到进出这两个带宽在物理上是隔离的啊，是不不不受影响。所以我们假设我们两两个传输都用同一台机器，
41:14我只要保证这两个传输的这个方向不同，那么我们就可以去实现这个方向不同。我们就可以实现就是说完全在物理上没有任何干扰的。那么怎么去实现这个干扰呢？其实我们就看，对吧？
41:28你比如说 prefer decode，那它的网络流向无非就是这个这个 prefer 要传一个 CPI 去给这个这个的。好。那么如果我要扩容的话，我其实是从 decodecode 这个机器去扩它，会比这个的机器去扩扩好。
41:41为什么？因为你 prefprefile 的机器，它的网络流向是出的。那我如果读取参数我也是个出的这个流量的话，那它两个会打架。那你对于这个 decode 来说，
41:49它它本身是呃这个这个进出是相反的，对不对？所以我们就是说我们这样来传的话，就是它一进出啊，这个东西是不相反。这样的话我们就可以把它网络流量打到接近四百这样的一个这个这个这个这个这个这个这个这个 GPS。
42:04那这样的话就可以就可以消除掉这样的一个这个这个见到这个问题，大家大家需要注意是对于一些最近的模型，比如说这个 GMIA 对吧？它的屁不太全小啊，这个其实就不是一个特别大的问题了。 OK 好，
42:16这只是第一点。第一点第二点就是说其实你想大家想 multicuast 的这样的一个问题，对吧？它其实并不是一个新的这个系统问题。它其实是个很经典的传统统超超散，也有这个 molticast 问题。
42:28那这个问题它到底传统的难点在哪？它难点是说我们要我们前面说的那个 ring 啊，它只是一个 herisst 这样的一个方案。这个方案在同构网络下，就是我所有节之间的网速一样啊，它的性能是是是是是是是能保值是最优的。
42:42但是一旦我这个节点之间的这个网速是不一样。比如说我们现在来看这个这个这这个这个现在数据信信器，对对吧？它其实是分成这个 skill up 和 scalill out，网络 skill out 就是我们说的这个 RDA，然后 scalill up 就是 AV，
42:55 link 啊，这些网速天然都是不一样的。那么在这种网络下的话，这个这个这个这个这个这个你的这怎么能快速生成一个这个 moliccuass pen 啊，这其实是一个啊非常非常大大的这样的一个问啊，非常大的一个这样这样的一个问题啊。
43:11其实其实传统的就是传统的这样一个方法。其实会告诉我们，就是你如果是一个这个异构网络下，就是网速不一样下你要去生成一个毛最优的毛利卡斯，它其实是一个 NP hard 的问题。那这个问题其实对我们的这个这个 skill up 实际际上非常友友好。
43:25比如说我我就 scskill up n 个节点，对吧？你告诉我，你要算一个 NP hard 的问题，去求解这个传输方案。那这个东西等你算出来，
43:32对吧？这个黄花菜都凉了 OK。所以呢这个时候的话，其实我们就需要干些什么事啊，我们就需要去做一些 curristic 这样的一个设计。其实大家这个具体的比较比较复杂，
43:43就大家感兴可以看看。对，个其实本质核心源，就是说我们会把这个 NV link 啊，就是 NV link 连接的点啊，把它把它抽象成一个超级巨大的这个节点。
43:53那这样的话，我传参数的时候，我其实只需要传到一个这个 NV link 的节点。然后在 NV link 节点，先用这个 NV link 广播就行了。因为这样的一个好呃，
44:01再一个观察，其实就是说你的这个 NV link 其实非常非常快啊。你用 NV link 去做这个参数广播，它其实啊基本上是没有这样的开销。所以我们只需要什么只需要把这个 RDV 的这一块给传的快一点就行了。 ok 所以说这个其实也是利用一些这样一个观察。
44:15 ok 好，然后做完这块之后，对吧？其实对于一些现在的这个数据中心的主流的这个架构啊，就是你的这个网速相对来说较快的话啊，我们这个这套方案其实基本上就已经 work 了啊，
44:26 work 很好。但是呢我们其实发现一个事情，就是说你的云厂商对吧？你有 code，它不是所有的这个 clard 都是一个非常快的网 ass。什什么意思？
44:35就是你的 code 它其实是有折旧的对吧？我不知道，我之前买了一个很旧的四零九零 pass ter，我不可能说我我部署了一个新的 h 一百 pass，我就把那个四零九零零 pass 扔掉了。就就导这个个果果，
44:45就是在云厂里里面确实还是有些部署啊，它的这个网速啊其实相对较慢的。其实我们可以可以很明显的看到，对吧？这个有的集群，它的这个集群线网速只有十二点五 GB，
44:56它其实比其他的强很多了。当然这个网速慢的话，它的这个租金也会低很多，对吧？所以说这个其实也是一种部署的这样的一个模式。那如果谁呀，
45:07那这个这个这如果你的这个这个这个这个这个这个这个这个这个这个这个这个网速很慢。那你你前面即使再加入这些 model class 优化，其实也没有什么用处。因为你的这个 momodel 的这个传输的这个实验啊，其实本质上是 bout light 在这个 model 的大小除以这个这个 benefits 上 anenefself。好，
45:25那么我们能不能解决这样一个方法呢？其实我们会发现有一种非常强妙非常强妙的这个这个 model 的这个 skills 方法能够实现。就是说我的这个 inserts 啊，它只 load 一部分参数，它就能够提升这个生命的吞吐。同时呢还不改任何的算法啊，
45:40注意它不是像现在很多那种优化，搞什么 early stop，对吧？就我模型算个三层，我就返回。不是那种东西都都改算法，
45:47都是都是基本上不不大可能在这个这个我很少会在这个 real diplomatment。那怎么做呢？怎么做呢？其实我们来看一下，对吧？其实现在的这样一套 del loading 的方法的一个核心的问题是吧？
45:59就是说我来了一个请求。好，我得等这个模型的这样的一个全部参数都漏 de，完了，我才能去做这个做这个这个服务，对不对？
46:08这个是现在模型 low 的这样一个问题。那聪明的大家可定都能想到，就是我其实是能做一些优化。比如说我这个模型传了一层了，我其实就可以把这一层计算给做掉，对不对？
46:19这样一种 overlap 的方式，其实是能够让这让这个这个这个这个这个时间啊做的更快的。大家这些都是现有的方法，比如说 pipe、 switch 啊，那么大家可以仔细想一想，
46:29是不是就是说如果我们是做一个大模型，就就是大模型的特征什么？就是它的参数很大，这种简单的 overlap 有没有效果？它其实没有效果，没有效果。
46:39为什么？因为大家想想，我们在模型，比如说我们做一次 prefuile，正常情况下大概是在两百毫秒的这样的延迟。但是如果我们要做这个 model 的所有的 load，
46:49这个 del load 它是大概是需要四到五秒的延迟。尤其你在慢的网络下，你快的网络下，它可能几百毫秒。那你慢的网络下讲几秒钟，几秒钟就是什么，
46:57你 overoverp 能减去的时间是什么？我最多减去的就是你的这个 computer 的时，对，对不对？那你这个的时间最多只能在几百几秒钟内减去两百毫秒。大家想这个数量来说的话，
47:07它其实帮助是非常非常小的意味着什么？因为说我们能观察到的一个事情，就是说你的整个系统啊必须得等到这个所有，还是得等到所有的参数加载完了。好，我的这只有这一刻吞吐才能上去吧。
47:20因为只有你所有的参数都加载完了，我的这个系统的这个 suput 才能有一个 bost。好，那么这个时候我们我们能不能做到一件事情，就是我这台机器对吧？它的这个参数只加载一部分，
47:34我就能把这两台机器的这个吞吐都都做上去。其实这个我们发现是可以做到的，可以做到的一个原因是什么呢？原因是就是说一般来说我要做扩容的时候，对吧？我一般这个这个厂商应该已经已经 deploy 的模型，
47:46就是它已经有一些 instance 在服务了。只不过这些服务的它的吞吐不够不够。那么这个时候我们其实是可以做一些协同，让他们去吞什么意思？比如说假设我原本有个 GPU 零在做服务，然后呢，
48:00我希望在 GPU 上扩一个模型。然后我们说你基于 natoo 的方法，其实就是我们是要把 GPU 一的这个这个这个这个这个这个这个这个这个这个这个这个呃就是要把参数从 GPU 呃呃从 GPU 啊，不是，是参参，从 GGUU 回来读到 GPU 一上 OK。
48:16好，但这个时候我们跟之前不同，之前漏的什么？就我把第零层第一层第二层第三层哎，逐步给漏了过来啊，我们做法跟别人不一样。
48:24我们是从第 n 层开始，就是我们先把第 n 层加载，然后再加载 n 减一层，再加载 n 减二层。然后一旦我这个 GPU 一啊加载了 n 减一层，这个数据，
48:33我就把我上面一个 GPU 零，它只需要算一到 n 减一层，它不需要算一层，全都算了。然后呢，它直接把最后一层交给 GPU 一算。
48:42然后呢，当我传了两层之后，我这个 GPU 零它只需要算一减 n 减二层就行。好，这样一种非常巧妙的这个拍 pad 的这种方式，它能够做到什么呢？
48:53就是我只要加载一层或者两层，我的整个系统的吞吐就能提升了。为什么？大家想想你的系统的吞吐取决于什么？取决于你这个这个一这个 GPU 零，它的这个做了多少层，
49:04对不对？我这个 GPU 零，它因为它的每这个 request，它的计算量其实是逐渐减小的，所以意味什么？它的每一个 request 算的其实会比你更快。
49:14所以一旦我们做了一层的这样的一个传递，它的这个 GQ 零的吞吐器就能上去了。对大家想想是不是这个是其实一个啊非常巧妙的这样的一个一个设计。当然它只是对慢速网络比较好。那这个概念的话，我们叫做 life，
49:28 or for 的话，这个东西也是我们一个提的对。然后有了这两个东西之后，对吧？其实我们就可以发现啊，像这种基于 auto scaling 的这种方式，
49:38其实是非常非常适合这样的一个这个啊 model 服务的啊，就是有比比起现像现在这种静态的这种方式啊，我们也其实比了一下这个这个比如下这个这个这个这个现有性的，比如像黑色，对吧？它像 so CM，
49:54其实我们可以看到的是啊相比较原本一个比如说我 GPU 百分之百用的这个场景啊，我们的这个 latency 啊，它其实只会牺牲大概百分之五到百分之十。但是我们相但是我们的整个 GPU 资源利用率的话，其实节省了百百分之百分之六十。然后相比较较这我我只有半半的这个部署的话，
50:12我的这个性能会好非常多。但是但是我的这个 GPU 率期反而会会更低 OK。所以说这个这个这样的话，这个这个这个事情其实就是嗯能够做到一个很快的事啊。所天你在休息吧，为什么？
50:30行？ b 懂不懂啊？我给你看这个团体没有的，用积金分积极。嗯嗯，那个你写的嗯嘘嗯嗯，
52:33是是是就是这种是嗯一开始截图，直接就嗯是嗯嘘。